{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "310e6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bca5f518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "391\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_val = x_train[50000:60000]\n",
    "x_train = x_train[0:50000]\n",
    "y_val = y_train[50000:60000]\n",
    "y_train = y_train[0:50000]\n",
    "x_train = x_train.astype(np.float32).reshape(-1,28,28,1) / 255.0\n",
    "x_val = x_val.astype(np.float32).reshape(-1,28,28,1) / 255.0\n",
    "x_test = x_test.astype(np.float32).reshape(-1,28,28,1) / 255.0\n",
    "y_train = tf.one_hot(y_train, depth=10)\n",
    "y_val = tf.one_hot(y_val, depth=10)\n",
    "y_test = tf.one_hot(y_test, depth=10)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_val.shape)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(128)\n",
    "train_dataset_full = train_dataset.shuffle(buffer_size=1024).batch(len(train_dataset))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(128)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(128)\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dae04d",
   "metadata": {},
   "source": [
    "#Create your custom CNN class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e71af1",
   "metadata": {},
   "source": [
    "Convolution layers has 4D weights of size (h,w,input_feature, output_feature), where h=height of your kernel and w = width of our kernel. If you add batches then it is 5D.\n",
    "Now your model will convolve across your input feature map with kernel and create output feature map, that is then passed to next layer.\n",
    "As we have learned in our prior class, to initialize your weights, we use tf.Variable(weight_init(size)), tf.keras.layers.Conv2D will do this for you. Play with the function and see how it works for your problem.\n",
    "Few important concepts, learn to save your model after every k epochs and start re-training from last checkpoint. This is very useful, and you don't need to retrain your model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338ba71",
   "metadata": {},
   "source": [
    "# CNN Pre-activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca1d715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRecognitionCNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_classes, device='cpu:0', checkpoint_directory=None):\n",
    "        ''' Define the parameterized layers used during forward-pass, the device\n",
    "            where you would like to run the computation (GPU, TPU, CPU) on and the checkpoint\n",
    "            directory.\n",
    "            \n",
    "            Args:\n",
    "                num_classes: the number of labels in the network.\n",
    "                device: string, 'cpu:n' or 'gpu:n' (n can vary). Default, 'cpu:0'.\n",
    "                checkpoint_directory: the directory where you would like to save or \n",
    "                                      restore a model.\n",
    "        ''' \n",
    "        super(ImageRecognitionCNN, self).__init__()\n",
    "        \n",
    "        # Initialize layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3,padding='same', activation=None)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D()\n",
    "        self.conv3 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)\n",
    "        self.conv4 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)\n",
    "        self.conv5 = tf.keras.layers.Conv2D(num_classes, 1, padding='same', activation=None)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn5 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        \n",
    "        # Define the device \n",
    "        self.device = device\n",
    "        \n",
    "        # Define the checkpoint directory\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        self.acc = tf.keras.metrics.Accuracy()\n",
    "\n",
    "\n",
    "    def predict(self, images, training):\n",
    "        \"\"\" Predicts the probability of each class, based on the input sample.\n",
    "            \n",
    "            Args:\n",
    "                images: 4D tensor. Either an image or a batch of images.\n",
    "                training: Boolean. Either the network is predicting in\n",
    "                          training mode or not.\n",
    "        \"\"\"\n",
    "        x = self.conv1(images)\n",
    "        x = self.bn1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv5(x)\n",
    "        #x = tf.nn.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = tf.reshape(x, (-1, 1, 10))\n",
    "        #x = tf.keras.layers.Flatten(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def loss_fn(self, images, target, training):\n",
    "        \"\"\" Defines the loss function used during \n",
    "            training.         \n",
    "        \"\"\"\n",
    "        preds = self.predict(images, training)\n",
    "        #print(preds.shape)\n",
    "        #print(target.shape)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=target, logits=preds)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def grads_fn(self, images, target, training):\n",
    "        \"\"\" Dynamically computes the gradients of the loss value\n",
    "            with respect to the parameters of the model, in each\n",
    "            forward pass.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_fn(images, target, training)\n",
    "        return tape.gradient(loss, self.variables)\n",
    "    \n",
    "    def restore_model(self):\n",
    "        \"\"\" Function to restore trained model.\n",
    "        \"\"\"\n",
    "        with tf.device(self.device):\n",
    "            # Run the model once to initialize variables\n",
    "            dummy_input = tf.constant(tf.zeros((1,48,48,1)))\n",
    "            dummy_pred = self.predict(dummy_input, training=False)\n",
    "            # Restore the variables of the model\n",
    "            saver = tf.Saver(self.variables)\n",
    "            saver.restore(tf.train.latest_checkpoint\n",
    "                          (self.checkpoint_directory))\n",
    "    \n",
    "    def save_model(self, global_step=0):\n",
    "        \"\"\" Function to save trained model.\n",
    "        \"\"\"\n",
    "        tf.Saver(self.variables).save(self.checkpoint_directory, \n",
    "                                       global_step=global_step)   \n",
    "    \n",
    "    # def compute_accuracy(self, input_data):\n",
    "    #     \"\"\" Compute the accuracy on the input data.\n",
    "    #     \"\"\"\n",
    "    #     with tf.device(self.device):\n",
    "    #         #acc = tf.metrics.Accuracy()\n",
    "    #         for step ,(images, targets) in enumerate(input_data):\n",
    "    #             # Predict the probability of each class\n",
    "    #             #print(targets.shape)\n",
    "    #             logits = self.predict(images, training=False)\n",
    "    #             # Select the class with the highest probability\n",
    "    #             #print(logits.shape)\n",
    "    #             logits = tf.nn.softmax(logits)\n",
    "    #             logits = tf.reshape(logits, [-1, 10])\n",
    "    #             targets = tf.reshape(targets, [-1,10])\n",
    "    #             preds = tf.argmax(logits, axis=1)\n",
    "                \n",
    "    #             #m1.update_state\n",
    "    #             # Compute the accuracy\n",
    "    #             #print(preds.shape)\n",
    "    #             acc(tf.reshape(targets, preds))\n",
    "    #     return acc\n",
    "\n",
    "    def compute_accuracy_2(self, images, targets):\n",
    "        \"\"\" Compute the accuracy on the input data.\n",
    "        \"\"\"\n",
    "        with tf.device(self.device):\n",
    "            \n",
    "            # Predict the probability of each class\n",
    "            logits = self.predict(images, training=False)\n",
    "            # Select the class with the highest probability\n",
    "            \n",
    "            logits = tf.nn.softmax(logits)\n",
    "            logits = tf.reshape(logits, [-1, 10])\n",
    "            targets = tf.reshape(targets, [-1,10])\n",
    "            preds = tf.argmax(logits, axis=1)\n",
    "            goal = tf.argmax(targets, axis=1)\n",
    "            self.acc.update_state(goal, preds)\n",
    "            # Compute the accuracy\n",
    "            result = self.acc.result().numpy()\n",
    "        return result\n",
    "\n",
    "  \n",
    "    def fit_fc(self, training_data, eval_data, test_data, optimizer, num_epochs=500, \n",
    "            early_stopping_rounds=10, verbose=10, train_from_scratch=False):\n",
    "        \"\"\" Function to train the model, using the selected optimizer and\n",
    "            for the desired number of epochs. You can either train from scratch\n",
    "            or load the latest model trained. Early stopping is used in order to\n",
    "            mitigate the risk of overfitting the network.\n",
    "            \n",
    "            Args:\n",
    "                training_data: the data you would like to train the model on.\n",
    "                                Must be in the tf.data.Dataset format.\n",
    "                eval_data: the data you would like to evaluate the model on.\n",
    "                            Must be in the tf.data.Dataset format.\n",
    "                optimizer: the optimizer used during training.\n",
    "                num_epochs: the maximum number of iterations you would like to \n",
    "                            train the model.\n",
    "                early_stopping_rounds: stop training if the loss on the eval \n",
    "                                       dataset does not decrease after n epochs.\n",
    "                verbose: int. Specify how often to print the loss value of the network.\n",
    "                train_from_scratch: boolean. Whether to initialize variables of the\n",
    "                                    the last trained model or initialize them\n",
    "                                    randomly.\n",
    "        \"\"\" \n",
    "    \n",
    "        if train_from_scratch==False:\n",
    "            self.restore_model()\n",
    "        \n",
    "        # Initialize best loss. This variable will store the lowest loss on the\n",
    "        # eval dataset.\n",
    "        best_loss = 999\n",
    "        \n",
    "        # Initialize classes to update the mean loss of train and eval\n",
    "        train_loss = tf.keras.metrics.Mean('train_loss')\n",
    "        eval_loss = tf.keras.metrics.Mean('eval_loss')\n",
    "        test_loss = tf.keras.metrics.Mean('test_loss')\n",
    "        acc_train = tf.keras.metrics.Mean('train_acc')\n",
    "        acc_val = tf.keras.metrics.Mean('val_acc')\n",
    "        acc_test = tf.keras.metrics.Mean('test_acc')\n",
    "        \n",
    "        # Initialize dictionary to store the loss history\n",
    "        self.history = {}\n",
    "        self.history['train_loss'] = []\n",
    "        self.history['eval_loss'] = []\n",
    "        self.history['test_loss'] = []\n",
    "        self.history['train_acc'] = []\n",
    "        self.history['val_acc'] = []\n",
    "        self.history['test_acc'] = []\n",
    "        \n",
    "        # Begin training\n",
    "        with tf.device(self.device):\n",
    "            for i in range(num_epochs):\n",
    "                # Training with gradient descent\n",
    "                #training_data_x = training_data.shuffle(buffer_size=1024).batch(128)\n",
    "                for step, (images, target) in enumerate(training_data):\n",
    "                    grads = self.grads_fn(images, target, True)\n",
    "                    optimizer.apply_gradients(zip(grads, self.variables))\n",
    "                    \n",
    "                # Compute the loss on the training data after one epoch\n",
    "                for step, (images, target) in enumerate(training_data):\n",
    "                    loss = self.loss_fn(images, target, False)\n",
    "                    accuracy = self.compute_accuracy_2(images,target)\n",
    "                    acc_train(accuracy)\n",
    "                    train_loss(loss)\n",
    "                self.history['train_loss'].append(train_loss.result().numpy())\n",
    "                self.history['train_acc'].append(acc_train.result().numpy())\n",
    "                # Reset metrics\n",
    "                train_loss.reset_states()\n",
    "                acc_train.reset_states()\n",
    "                \n",
    "                # Compute the loss on the eval data after one epoch\n",
    "                for step, (images, target) in enumerate(eval_data):\n",
    "                    loss = self.loss_fn(images, target, False)\n",
    "                    accuracy = self.compute_accuracy_2(images,target)\n",
    "                    acc_val(accuracy)\n",
    "                    eval_loss(loss)\n",
    "                self.history['eval_loss'].append(eval_loss.result().numpy())\n",
    "                self.history['val_acc'].append(acc_val.result().numpy())\n",
    "                # Reset metrics\n",
    "                eval_loss.reset_states()\n",
    "                acc_val.reset_states()\n",
    "                \n",
    "                # Compute the loss on the test data after one epoch\n",
    "                for step, (images, target) in enumerate(test_data):\n",
    "                    loss = self.loss_fn(images, target, False)\n",
    "                    accuracy = self.compute_accuracy_2(images,target)\n",
    "                    acc_val(accuracy)\n",
    "                    eval_loss(loss)\n",
    "                self.history['test_loss'].append(eval_loss.result().numpy())\n",
    "                self.history['test_acc'].append(acc_val.result().numpy())\n",
    "                # Reset metrics\n",
    "                eval_loss.reset_states()\n",
    "                acc_val.reset_states()\n",
    "                \n",
    "                # Print train and eval losses\n",
    "                if ((i+1)%verbose==0):\n",
    "                    print('Train loss at epoch %d: ' %(i+1), self.history['train_loss'][-1])\n",
    "                    print('Train Acc at epoch %d: ' %(i+1), self.history['train_acc'][-1])\n",
    "                    \n",
    "                    print('Eval loss at epoch %d: ' %(i+1), self.history['eval_loss'][-1])\n",
    "                    print('Eval Acc at epoch %d: ' %(i+1), self.history['val_acc'][-1])\n",
    "                    \n",
    "                    print('Test loss at epoch %d: ' %(i+1), self.history['test_loss'][-1])\n",
    "                    print('Test Acc at epoch %d: ' %(i+1), self.history['test_acc'][-1])\n",
    "\n",
    "\n",
    "                # Check for early stopping\n",
    "                if self.history['eval_loss'][-1]<best_loss:\n",
    "                    best_loss = self.history['eval_loss'][-1]\n",
    "                    count = early_stopping_rounds\n",
    "                else:\n",
    "                    count -= 1\n",
    "                if count==0:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f6ccf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at epoch 10:  0.33087292\n",
      "Train Acc at epoch 10:  0.84014887\n",
      "Eval loss at epoch 10:  0.34981146\n",
      "Eval Acc at epoch 10:  0.84202605\n",
      "Test loss at epoch 10:  0.36585197\n",
      "Test Acc at epoch 10:  0.84248257\n",
      "Train loss at epoch 20:  0.27796355\n",
      "Train Acc at epoch 20:  0.86572766\n",
      "Eval loss at epoch 20:  0.3036277\n",
      "Eval Acc at epoch 20:  0.8664825\n",
      "Test loss at epoch 20:  0.32077485\n",
      "Test Acc at epoch 20:  0.86665064\n",
      "Train loss at epoch 30:  0.23980525\n",
      "Train Acc at epoch 30:  0.87889546\n",
      "Eval loss at epoch 30:  0.27492082\n",
      "Eval Acc at epoch 30:  0.8793816\n",
      "Test loss at epoch 30:  0.29431093\n",
      "Test Acc at epoch 30:  0.87946945\n",
      "Train loss at epoch 40:  0.20356004\n",
      "Train Acc at epoch 40:  0.88794935\n",
      "Eval loss at epoch 40:  0.24889302\n",
      "Eval Acc at epoch 40:  0.8883559\n",
      "Test loss at epoch 40:  0.26908845\n",
      "Test Acc at epoch 40:  0.8884201\n",
      "Train loss at epoch 50:  0.19313686\n",
      "Train Acc at epoch 50:  0.89467144\n",
      "Eval loss at epoch 50:  0.25152716\n",
      "Eval Acc at epoch 50:  0.8949538\n",
      "Test loss at epoch 50:  0.27321342\n",
      "Test Acc at epoch 50:  0.8949862\n",
      "Train loss at epoch 60:  0.1641901\n",
      "Train Acc at epoch 60:  0.90045065\n",
      "Eval loss at epoch 60:  0.23952906\n",
      "Eval Acc at epoch 60:  0.9007238\n",
      "Test loss at epoch 60:  0.25858548\n",
      "Test Acc at epoch 60:  0.9007455\n",
      "Train loss at epoch 70:  0.14786135\n",
      "Train Acc at epoch 70:  0.9052102\n",
      "Eval loss at epoch 70:  0.23839556\n",
      "Eval Acc at epoch 70:  0.9054427\n",
      "Test loss at epoch 70:  0.2572721\n",
      "Test Acc at epoch 70:  0.90545577\n",
      "Train loss at epoch 80:  0.14424835\n",
      "Train Acc at epoch 80:  0.9094222\n",
      "Eval loss at epoch 80:  0.25355095\n",
      "Eval Acc at epoch 80:  0.90959865\n",
      "Test loss at epoch 80:  0.27889273\n",
      "Test Acc at epoch 80:  0.9095987\n",
      "Train loss at epoch 90:  0.12701678\n",
      "Train Acc at epoch 90:  0.9131522\n",
      "Eval loss at epoch 90:  0.25484502\n",
      "Eval Acc at epoch 90:  0.9133198\n",
      "Test loss at epoch 90:  0.28020337\n",
      "Test Acc at epoch 90:  0.9133159\n",
      "Train loss at epoch 100:  0.101193145\n",
      "Train Acc at epoch 100:  0.9164626\n",
      "Eval loss at epoch 100:  0.2527529\n",
      "Eval Acc at epoch 100:  0.9166421\n",
      "Test loss at epoch 100:  0.27825907\n",
      "Test Acc at epoch 100:  0.91663873\n",
      "\n",
      "Total time taken (in seconds): 9766.33\n",
      "Train loss at epoch 10:  0.37098208\n",
      "Train Acc at epoch 10:  0.83688766\n",
      "Eval loss at epoch 10:  0.38528565\n",
      "Eval Acc at epoch 10:  0.8381586\n",
      "Test loss at epoch 10:  0.4054881\n",
      "Test Acc at epoch 10:  0.8384691\n",
      "Train loss at epoch 20:  0.28108937\n",
      "Train Acc at epoch 20:  0.8612398\n",
      "Eval loss at epoch 20:  0.30669478\n",
      "Eval Acc at epoch 20:  0.86201495\n",
      "Test loss at epoch 20:  0.32420537\n",
      "Test Acc at epoch 20:  0.862172\n",
      "Train loss at epoch 30:  0.24056935\n",
      "Train Acc at epoch 30:  0.87456083\n",
      "Eval loss at epoch 30:  0.27572677\n",
      "Eval Acc at epoch 30:  0.87509936\n",
      "Test loss at epoch 30:  0.2917866\n",
      "Test Acc at epoch 30:  0.87521005\n",
      "Train loss at epoch 40:  0.21665488\n",
      "Train Acc at epoch 40:  0.88406146\n",
      "Eval loss at epoch 40:  0.26370186\n",
      "Eval Acc at epoch 40:  0.8844474\n",
      "Test loss at epoch 40:  0.2818911\n",
      "Test Acc at epoch 40:  0.8845096\n",
      "Train loss at epoch 50:  0.19653209\n",
      "Train Acc at epoch 50:  0.8914126\n",
      "Eval loss at epoch 50:  0.25923115\n",
      "Eval Acc at epoch 50:  0.89171207\n",
      "Test loss at epoch 50:  0.27730405\n",
      "Test Acc at epoch 50:  0.89174324\n",
      "Train loss at epoch 60:  0.17317578\n",
      "Train Acc at epoch 60:  0.89738697\n",
      "Eval loss at epoch 60:  0.24651231\n",
      "Eval Acc at epoch 60:  0.8976538\n",
      "Test loss at epoch 60:  0.26665786\n",
      "Test Acc at epoch 60:  0.8976763\n",
      "Train loss at epoch 70:  0.15087743\n",
      "Train Acc at epoch 70:  0.90242505\n",
      "Eval loss at epoch 70:  0.23914078\n",
      "Eval Acc at epoch 70:  0.9026699\n",
      "Test loss at epoch 70:  0.2597556\n",
      "Test Acc at epoch 70:  0.9026919\n",
      "Train loss at epoch 80:  0.13580517\n",
      "Train Acc at epoch 80:  0.90685993\n",
      "Eval loss at epoch 80:  0.2391661\n",
      "Eval Acc at epoch 80:  0.90707445\n",
      "Test loss at epoch 80:  0.2594698\n",
      "Test Acc at epoch 80:  0.90708596\n",
      "Train loss at epoch 90:  0.13323686\n",
      "Train Acc at epoch 90:  0.9103772\n",
      "Eval loss at epoch 90:  0.25806293\n",
      "Eval Acc at epoch 90:  0.91054815\n",
      "Test loss at epoch 90:  0.27969927\n",
      "Test Acc at epoch 90:  0.9105478\n",
      "Train loss at epoch 100:  0.10870084\n",
      "Train Acc at epoch 100:  0.9135541\n",
      "Eval loss at epoch 100:  0.25164613\n",
      "Eval Acc at epoch 100:  0.91373074\n",
      "Test loss at epoch 100:  0.27210057\n",
      "Test Acc at epoch 100:  0.91373134\n",
      "\n",
      "Total time taken (in seconds): 39386.57\n",
      "Train loss at epoch 10:  0.32107735\n",
      "Train Acc at epoch 10:  0.84471846\n",
      "Eval loss at epoch 10:  0.3403463\n",
      "Eval Acc at epoch 10:  0.84643644\n",
      "Test loss at epoch 10:  0.36034623\n",
      "Test Acc at epoch 10:  0.8468449\n",
      "Train loss at epoch 20:  0.2710625\n",
      "Train Acc at epoch 20:  0.8674215\n",
      "Eval loss at epoch 20:  0.30191255\n",
      "Eval Acc at epoch 20:  0.8681426\n",
      "Test loss at epoch 20:  0.31831217\n",
      "Test Acc at epoch 20:  0.8683048\n",
      "Train loss at epoch 30:  0.23226452\n",
      "Train Acc at epoch 30:  0.8800485\n",
      "Eval loss at epoch 30:  0.27110648\n",
      "Eval Acc at epoch 30:  0.8805449\n",
      "Test loss at epoch 30:  0.29019454\n",
      "Test Acc at epoch 30:  0.8806332\n",
      "Train loss at epoch 40:  0.2153924\n",
      "Train Acc at epoch 40:  0.8885737\n",
      "Eval loss at epoch 40:  0.27046657\n",
      "Eval Acc at epoch 40:  0.8888882\n",
      "Test loss at epoch 40:  0.2897144\n",
      "Test Acc at epoch 40:  0.888921\n",
      "Train loss at epoch 50:  0.19090931\n",
      "Train Acc at epoch 50:  0.8947607\n",
      "Eval loss at epoch 50:  0.25404707\n",
      "Eval Acc at epoch 50:  0.8950413\n",
      "Test loss at epoch 50:  0.27768424\n",
      "Test Acc at epoch 50:  0.8950651\n",
      "Train loss at epoch 60:  0.16685118\n",
      "Train Acc at epoch 60:  0.8997505\n",
      "Eval loss at epoch 60:  0.24779119\n",
      "Eval Acc at epoch 60:  0.9000149\n",
      "Test loss at epoch 60:  0.27119666\n",
      "Test Acc at epoch 60:  0.9000383\n",
      "Train loss at epoch 70:  0.1462972\n",
      "Train Acc at epoch 70:  0.9043147\n",
      "Eval loss at epoch 70:  0.24236116\n",
      "Eval Acc at epoch 70:  0.9045564\n",
      "Test loss at epoch 70:  0.2695575\n",
      "Test Acc at epoch 70:  0.9045684\n",
      "Train loss at epoch 80:  0.13746166\n",
      "Train Acc at epoch 80:  0.90838116\n",
      "Eval loss at epoch 80:  0.25018898\n",
      "Eval Acc at epoch 80:  0.9085754\n",
      "Test loss at epoch 80:  0.2769817\n",
      "Test Acc at epoch 80:  0.9085761\n",
      "Train loss at epoch 90:  0.14545007\n",
      "Train Acc at epoch 90:  0.9118387\n",
      "Eval loss at epoch 90:  0.28642613\n",
      "Eval Acc at epoch 90:  0.91197705\n",
      "Test loss at epoch 90:  0.30713823\n",
      "Test Acc at epoch 90:  0.9119579\n",
      "Train loss at epoch 100:  0.11319259\n",
      "Train Acc at epoch 100:  0.9150025\n",
      "Eval loss at epoch 100:  0.2707512\n",
      "Eval Acc at epoch 100:  0.9151638\n",
      "Test loss at epoch 100:  0.29686782\n",
      "Test Acc at epoch 100:  0.91515523\n",
      "\n",
      "Total time taken (in seconds): 9164.85\n"
     ]
    }
   ],
   "source": [
    "# Specify the path where you want to save/restore the trained variables.\n",
    "checkpoint_directory = 'models_checkpoints/mnist/'\n",
    "\n",
    "# Use the GPU if available.\n",
    "device = 'gpu:0'\n",
    "\n",
    "for i in range(3):\n",
    "    time_start = time.time()\n",
    "    np.random.seed(i)\n",
    "    tf.random.set_seed(i)\n",
    "    # Define optimizer.\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4)\n",
    "\n",
    "    # Instantiate model. This doesn't initialize the variables yet.\n",
    "    model = ImageRecognitionCNN(num_classes=10, device=device, \n",
    "                                  checkpoint_directory=checkpoint_directory)\n",
    "\n",
    "    #model = ImageRecognitionCNN(num_classes=7, device=device)\n",
    "    # Train model\n",
    "    model.fit_fc(train_dataset, val_dataset, test_dataset, optimizer, num_epochs=100, \n",
    "              early_stopping_rounds=100, verbose=10, train_from_scratch=True)\n",
    "    time_taken = time.time() - time_start \n",
    "    print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58812a8",
   "metadata": {},
   "source": [
    "# Post-activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "152c44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRecognitionCNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_classes, device='cpu:0', checkpoint_directory=None):\n",
    "        ''' Define the parameterized layers used during forward-pass, the device\n",
    "            where you would like to run the computation (GPU, TPU, CPU) on and the checkpoint\n",
    "            directory.\n",
    "            \n",
    "            Args:\n",
    "                num_classes: the number of labels in the network.\n",
    "                device: string, 'cpu:n' or 'gpu:n' (n can vary). Default, 'cpu:0'.\n",
    "                checkpoint_directory: the directory where you would like to save or \n",
    "                                      restore a model.\n",
    "        ''' \n",
    "        super(ImageRecognitionCNN, self).__init__()\n",
    "        \n",
    "        # Initialize layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 3,padding='same', activation=None)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D()\n",
    "        self.conv3 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)\n",
    "        self.conv4 = tf.keras.layers.Conv2D(64, 3, padding='same', activation=None)\n",
    "        self.conv5 = tf.keras.layers.Conv2D(num_classes, 1, padding='same', activation=None)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        self.bn5 = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=0.1)\n",
    "        \n",
    "        # Define the device \n",
    "        self.device = device\n",
    "        \n",
    "        # Define the checkpoint directory\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        self.acc = tf.keras.metrics.Accuracy()\n",
    "\n",
    "\n",
    "    def predict(self, images, training):\n",
    "        \"\"\" Predicts the probability of each class, based on the input sample.\n",
    "            \n",
    "            Args:\n",
    "                images: 4D tensor. Either an image or a batch of images.\n",
    "                training: Boolean. Either the network is predicting in\n",
    "                          training mode or not.\n",
    "        \"\"\"\n",
    "        x = self.conv1(images)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv5(x)\n",
    "        #x = tf.nn.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = tf.reshape(x, (-1, 1, 10))\n",
    "        #x = tf.keras.layers.Flatten(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def loss_fn(self, images, target, training):\n",
    "        \"\"\" Defines the loss function used during \n",
    "            training.         \n",
    "        \"\"\"\n",
    "        preds = self.predict(images, training)\n",
    "        #print(preds.shape)\n",
    "        #print(target.shape)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=target, logits=preds)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def grads_fn(self, images, target, training):\n",
    "        \"\"\" Dynamically computes the gradients of the loss value\n",
    "            with respect to the parameters of the model, in each\n",
    "            forward pass.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_fn(images, target, training)\n",
    "        return tape.gradient(loss, self.variables)\n",
    "    \n",
    "    def restore_model(self):\n",
    "        \"\"\" Function to restore trained model.\n",
    "        \"\"\"\n",
    "        with tf.device(self.device):\n",
    "            # Run the model once to initialize variables\n",
    "            dummy_input = tf.constant(tf.zeros((1,48,48,1)))\n",
    "            dummy_pred = self.predict(dummy_input, training=False)\n",
    "            # Restore the variables of the model\n",
    "            saver = tf.Saver(self.variables)\n",
    "            saver.restore(tf.train.latest_checkpoint\n",
    "                          (self.checkpoint_directory))\n",
    "    \n",
    "    def save_model(self, global_step=0):\n",
    "        \"\"\" Function to save trained model.\n",
    "        \"\"\"\n",
    "        tf.Saver(self.variables).save(self.checkpoint_directory, \n",
    "                                       global_step=global_step)   \n",
    "    \n",
    "    # def compute_accuracy(self, input_data):\n",
    "    #     \"\"\" Compute the accuracy on the input data.\n",
    "    #     \"\"\"\n",
    "    #     with tf.device(self.device):\n",
    "    #         #acc = tf.metrics.Accuracy()\n",
    "    #         for step ,(images, targets) in enumerate(input_data):\n",
    "    #             # Predict the probability of each class\n",
    "    #             #print(targets.shape)\n",
    "    #             logits = self.predict(images, training=False)\n",
    "    #             # Select the class with the highest probability\n",
    "    #             #print(logits.shape)\n",
    "    #             logits = tf.nn.softmax(logits)\n",
    "    #             logits = tf.reshape(logits, [-1, 10])\n",
    "    #             targets = tf.reshape(targets, [-1,10])\n",
    "    #             preds = tf.argmax(logits, axis=1)\n",
    "                \n",
    "    #             #m1.update_state\n",
    "    #             # Compute the accuracy\n",
    "    #             #print(preds.shape)\n",
    "    #             acc(tf.reshape(targets, preds))\n",
    "    #     return acc\n",
    "\n",
    "    def compute_accuracy_2(self, images, targets):\n",
    "        \"\"\" Compute the accuracy on the input data.\n",
    "        \"\"\"\n",
    "        with tf.device(self.device):\n",
    "            \n",
    "            # Predict the probability of each class\n",
    "            logits = self.predict(images, training=False)\n",
    "            # Select the class with the highest probability\n",
    "            \n",
    "            logits = tf.nn.softmax(logits)\n",
    "            logits = tf.reshape(logits, [-1, 10])\n",
    "            targets = tf.reshape(targets, [-1,10])\n",
    "            preds = tf.argmax(logits, axis=1)\n",
    "            goal = tf.argmax(targets, axis=1)\n",
    "            self.acc.update_state(goal, preds)\n",
    "            # Compute the accuracy\n",
    "            result = self.acc.result().numpy()\n",
    "        return result\n",
    "\n",
    "  \n",
    "    def fit_fc(self, training_data, eval_data, test_data, optimizer, num_epochs=500, \n",
    "            early_stopping_rounds=10, verbose=10, train_from_scratch=False):\n",
    "        \"\"\" Function to train the model, using the selected optimizer and\n",
    "            for the desired number of epochs. You can either train from scratch\n",
    "            or load the latest model trained. Early stopping is used in order to\n",
    "            mitigate the risk of overfitting the network.\n",
    "            \n",
    "            Args:\n",
    "                training_data: the data you would like to train the model on.\n",
    "                                Must be in the tf.data.Dataset format.\n",
    "                eval_data: the data you would like to evaluate the model on.\n",
    "                            Must be in the tf.data.Dataset format.\n",
    "                optimizer: the optimizer used during training.\n",
    "                num_epochs: the maximum number of iterations you would like to \n",
    "                            train the model.\n",
    "                early_stopping_rounds: stop training if the loss on the eval \n",
    "                                       dataset does not decrease after n epochs.\n",
    "                verbose: int. Specify how often to print the loss value of the network.\n",
    "                train_from_scratch: boolean. Whether to initialize variables of the\n",
    "                                    the last trained model or initialize them\n",
    "                                    randomly.\n",
    "        \"\"\" \n",
    "    \n",
    "        if train_from_scratch==False:\n",
    "            self.restore_model()\n",
    "        \n",
    "        # Initialize best loss. This variable will store the lowest loss on the\n",
    "        # eval dataset.\n",
    "        best_loss = 999\n",
    "        \n",
    "        # Initialize classes to update the mean loss of train and eval\n",
    "        train_loss = tf.keras.metrics.Mean('train_loss')\n",
    "        eval_loss = tf.keras.metrics.Mean('eval_loss')\n",
    "        test_loss = tf.keras.metrics.Mean('test_loss')\n",
    "        acc_train = tf.keras.metrics.Mean('train_acc')\n",
    "        acc_val = tf.keras.metrics.Mean('val_acc')\n",
    "        acc_test = tf.keras.metrics.Mean('test_acc')\n",
    "        \n",
    "        # Initialize dictionary to store the loss history\n",
    "        self.history = {}\n",
    "        self.history['train_loss'] = []\n",
    "        self.history['eval_loss'] = []\n",
    "        self.history['test_loss'] = []\n",
    "        self.history['train_acc'] = []\n",
    "        self.history['val_acc'] = []\n",
    "        self.history['test_acc'] = []\n",
    "        \n",
    "        # Begin training\n",
    "        with tf.device(self.device):\n",
    "            for i in range(num_epochs):\n",
    "                # Training with gradient descent\n",
    "                #training_data_x = training_data.shuffle(buffer_size=1024).batch(128)\n",
    "                for step, (images, target) in enumerate(training_data):\n",
    "                    grads = self.grads_fn(images, target, True)\n",
    "                    optimizer.apply_gradients(zip(grads, self.variables))\n",
    "                    \n",
    "                # Compute the loss on the training data after one epoch\n",
    "                for step, (images, target) in enumerate(training_data):\n",
    "                    loss = self.loss_fn(images, target, False)\n",
    "                    accuracy = self.compute_accuracy_2(images,target)\n",
    "                    acc_train(accuracy)\n",
    "                    train_loss(loss)\n",
    "                self.history['train_loss'].append(train_loss.result().numpy())\n",
    "                self.history['train_acc'].append(acc_train.result().numpy())\n",
    "                # Reset metrics\n",
    "                train_loss.reset_states()\n",
    "                acc_train.reset_states()\n",
    "                \n",
    "                # Compute the loss on the eval data after one epoch\n",
    "                for step, (images, target) in enumerate(eval_data):\n",
    "                    loss = self.loss_fn(images, target, False)\n",
    "                    accuracy = self.compute_accuracy_2(images,target)\n",
    "                    acc_val(accuracy)\n",
    "                    eval_loss(loss)\n",
    "                self.history['eval_loss'].append(eval_loss.result().numpy())\n",
    "                self.history['val_acc'].append(acc_val.result().numpy())\n",
    "                # Reset metrics\n",
    "                eval_loss.reset_states()\n",
    "                acc_val.reset_states()\n",
    "                \n",
    "                # Compute the loss on the test data after one epoch\n",
    "                for step, (images, target) in enumerate(test_data):\n",
    "                    loss = self.loss_fn(images, target, False)\n",
    "                    accuracy = self.compute_accuracy_2(images,target)\n",
    "                    acc_val(accuracy)\n",
    "                    eval_loss(loss)\n",
    "                self.history['test_loss'].append(eval_loss.result().numpy())\n",
    "                self.history['test_acc'].append(acc_val.result().numpy())\n",
    "                # Reset metrics\n",
    "                eval_loss.reset_states()\n",
    "                acc_val.reset_states()\n",
    "                \n",
    "                # Print train and eval losses\n",
    "                if ((i+1)%verbose==0):\n",
    "                    print('Train loss at epoch %d: ' %(i+1), self.history['train_loss'][-1])\n",
    "                    print('Train Acc at epoch %d: ' %(i+1), self.history['train_acc'][-1])\n",
    "                    \n",
    "                    print('Eval loss at epoch %d: ' %(i+1), self.history['eval_loss'][-1])\n",
    "                    print('Eval Acc at epoch %d: ' %(i+1), self.history['val_acc'][-1])\n",
    "                    \n",
    "                    print('Test loss at epoch %d: ' %(i+1), self.history['test_loss'][-1])\n",
    "                    print('Test Acc at epoch %d: ' %(i+1), self.history['test_acc'][-1])\n",
    "\n",
    "\n",
    "                # Check for early stopping\n",
    "                if self.history['eval_loss'][-1]<best_loss:\n",
    "                    best_loss = self.history['eval_loss'][-1]\n",
    "                    count = early_stopping_rounds\n",
    "                else:\n",
    "                    count -= 1\n",
    "                if count==0:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bf9c30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at epoch 10:  0.33672792\n",
      "Train Acc at epoch 10:  0.84115475\n",
      "Eval loss at epoch 10:  0.35455072\n",
      "Eval Acc at epoch 10:  0.84281206\n",
      "Test loss at epoch 10:  0.3745774\n",
      "Test Acc at epoch 10:  0.8432102\n",
      "Train loss at epoch 20:  0.2737925\n",
      "Train Acc at epoch 20:  0.86625475\n",
      "Eval loss at epoch 20:  0.30431718\n",
      "Eval Acc at epoch 20:  0.8669841\n",
      "Test loss at epoch 20:  0.3198954\n",
      "Test Acc at epoch 20:  0.8671171\n",
      "Train loss at epoch 30:  0.2310382\n",
      "Train Acc at epoch 30:  0.8789357\n",
      "Eval loss at epoch 30:  0.27085468\n",
      "Eval Acc at epoch 30:  0.87947017\n",
      "Test loss at epoch 30:  0.28924063\n",
      "Test Acc at epoch 30:  0.87956935\n",
      "Train loss at epoch 40:  0.2041869\n",
      "Train Acc at epoch 40:  0.88821226\n",
      "Eval loss at epoch 40:  0.25471237\n",
      "Eval Acc at epoch 40:  0.8886059\n",
      "Test loss at epoch 40:  0.27350113\n",
      "Test Acc at epoch 40:  0.8886678\n",
      "Train loss at epoch 50:  0.17983131\n",
      "Train Acc at epoch 50:  0.89546084\n",
      "Eval loss at epoch 50:  0.24683619\n",
      "Eval Acc at epoch 50:  0.89577705\n",
      "Test loss at epoch 50:  0.26408005\n",
      "Test Acc at epoch 50:  0.8958122\n",
      "Train loss at epoch 60:  0.16103126\n",
      "Train Acc at epoch 60:  0.90133953\n",
      "Eval loss at epoch 60:  0.2416062\n",
      "Eval Acc at epoch 60:  0.9016104\n",
      "Test loss at epoch 60:  0.2626905\n",
      "Test Acc at epoch 60:  0.90163684\n",
      "Train loss at epoch 70:  0.138833\n",
      "Train Acc at epoch 70:  0.90633446\n",
      "Eval loss at epoch 70:  0.2393668\n",
      "Eval Acc at epoch 70:  0.90657794\n",
      "Test loss at epoch 70:  0.25921372\n",
      "Test Acc at epoch 70:  0.90658927\n",
      "Train loss at epoch 80:  0.12717713\n",
      "Train Acc at epoch 80:  0.91056854\n",
      "Eval loss at epoch 80:  0.24870346\n",
      "Eval Acc at epoch 80:  0.91078496\n",
      "Test loss at epoch 80:  0.26704645\n",
      "Test Acc at epoch 80:  0.91079015\n",
      "Train loss at epoch 90:  0.1094875\n",
      "Train Acc at epoch 90:  0.9144324\n",
      "Eval loss at epoch 90:  0.25622758\n",
      "Eval Acc at epoch 90:  0.91462255\n",
      "Test loss at epoch 90:  0.2727575\n",
      "Test Acc at epoch 90:  0.9146198\n",
      "Train loss at epoch 100:  0.09624547\n",
      "Train Acc at epoch 100:  0.91782755\n",
      "Eval loss at epoch 100:  0.26392066\n",
      "Eval Acc at epoch 100:  0.91801184\n",
      "Test loss at epoch 100:  0.28754237\n",
      "Test Acc at epoch 100:  0.9180088\n",
      "\n",
      "Total time taken (in seconds): 17206.46\n",
      "Train loss at epoch 10:  0.33727488\n",
      "Train Acc at epoch 10:  0.8335495\n",
      "Eval loss at epoch 10:  0.3577501\n",
      "Eval Acc at epoch 10:  0.8355904\n",
      "Test loss at epoch 10:  0.3730355\n",
      "Test Acc at epoch 10:  0.8361002\n",
      "Train loss at epoch 20:  0.27376053\n",
      "Train Acc at epoch 20:  0.86161923\n",
      "Eval loss at epoch 20:  0.29830825\n",
      "Eval Acc at epoch 20:  0.86248076\n",
      "Test loss at epoch 20:  0.31611153\n",
      "Test Acc at epoch 20:  0.8626827\n",
      "Train loss at epoch 30:  0.23335518\n",
      "Train Acc at epoch 30:  0.8761072\n",
      "Eval loss at epoch 30:  0.26613915\n",
      "Eval Acc at epoch 30:  0.876659\n",
      "Test loss at epoch 30:  0.286199\n",
      "Test Acc at epoch 30:  0.8767783\n",
      "Train loss at epoch 40:  0.21511196\n",
      "Train Acc at epoch 40:  0.88547665\n",
      "Eval loss at epoch 40:  0.25847906\n",
      "Eval Acc at epoch 40:  0.88586724\n",
      "Test loss at epoch 40:  0.2807523\n",
      "Test Acc at epoch 40:  0.8859282\n",
      "Train loss at epoch 50:  0.18841799\n",
      "Train Acc at epoch 50:  0.89266044\n",
      "Eval loss at epoch 50:  0.24885908\n",
      "Eval Acc at epoch 50:  0.8929679\n",
      "Test loss at epoch 50:  0.2673594\n",
      "Test Acc at epoch 50:  0.8930077\n",
      "Train loss at epoch 60:  0.16730659\n",
      "Train Acc at epoch 60:  0.89844805\n",
      "Eval loss at epoch 60:  0.2426952\n",
      "Eval Acc at epoch 60:  0.8987207\n",
      "Test loss at epoch 60:  0.26322326\n",
      "Test Acc at epoch 60:  0.8987514\n",
      "Train loss at epoch 70:  0.14902745\n",
      "Train Acc at epoch 70:  0.90334487\n",
      "Eval loss at epoch 70:  0.24164028\n",
      "Eval Acc at epoch 70:  0.90359247\n",
      "Test loss at epoch 70:  0.26041\n",
      "Test Acc at epoch 70:  0.9036116\n",
      "Train loss at epoch 80:  0.1305407\n",
      "Train Acc at epoch 80:  0.9076354\n",
      "Eval loss at epoch 80:  0.24131985\n",
      "Eval Acc at epoch 80:  0.9078573\n",
      "Test loss at epoch 80:  0.26324522\n",
      "Test Acc at epoch 80:  0.9078716\n",
      "Train loss at epoch 90:  0.114882044\n",
      "Train Acc at epoch 90:  0.9116774\n",
      "Eval loss at epoch 90:  0.24690683\n",
      "Eval Acc at epoch 90:  0.9118842\n",
      "Test loss at epoch 90:  0.27024162\n",
      "Test Acc at epoch 90:  0.911888\n",
      "Train loss at epoch 100:  0.12909585\n",
      "Train Acc at epoch 100:  0.9152073\n",
      "Eval loss at epoch 100:  0.28679302\n",
      "Eval Acc at epoch 100:  0.915336\n",
      "Test loss at epoch 100:  0.30359492\n",
      "Test Acc at epoch 100:  0.9153249\n",
      "\n",
      "Total time taken (in seconds): 12138.18\n",
      "Train loss at epoch 10:  0.32302883\n",
      "Train Acc at epoch 10:  0.8452802\n",
      "Eval loss at epoch 10:  0.3459834\n",
      "Eval Acc at epoch 10:  0.84694576\n",
      "Test loss at epoch 10:  0.364132\n",
      "Test Acc at epoch 10:  0.84732413\n",
      "Train loss at epoch 20:  0.27305958\n",
      "Train Acc at epoch 20:  0.8680768\n",
      "Eval loss at epoch 20:  0.30722392\n",
      "Eval Acc at epoch 20:  0.8687661\n",
      "Test loss at epoch 20:  0.32359016\n",
      "Test Acc at epoch 20:  0.8688943\n",
      "Train loss at epoch 30:  0.23589092\n",
      "Train Acc at epoch 30:  0.87984395\n",
      "Eval loss at epoch 30:  0.27928263\n",
      "Eval Acc at epoch 30:  0.8803171\n",
      "Test loss at epoch 30:  0.29620126\n",
      "Test Acc at epoch 30:  0.8804046\n",
      "Train loss at epoch 40:  0.20201863\n",
      "Train Acc at epoch 40:  0.88825417\n",
      "Eval loss at epoch 40:  0.26240164\n",
      "Eval Acc at epoch 40:  0.8886529\n",
      "Test loss at epoch 40:  0.27804127\n",
      "Test Acc at epoch 40:  0.88871247\n",
      "Train loss at epoch 50:  0.19431974\n",
      "Train Acc at epoch 50:  0.8948299\n",
      "Eval loss at epoch 50:  0.26631278\n",
      "Eval Acc at epoch 50:  0.8950963\n",
      "Test loss at epoch 50:  0.28461456\n",
      "Test Acc at epoch 50:  0.89511967\n",
      "Train loss at epoch 60:  0.1648656\n",
      "Train Acc at epoch 60:  0.90040755\n",
      "Eval loss at epoch 60:  0.2548054\n",
      "Eval Acc at epoch 60:  0.9006674\n",
      "Test loss at epoch 60:  0.27332076\n",
      "Test Acc at epoch 60:  0.90067977\n",
      "Train loss at epoch 70:  0.14627323\n",
      "Train Acc at epoch 70:  0.90499455\n",
      "Eval loss at epoch 70:  0.2568688\n",
      "Eval Acc at epoch 70:  0.90523326\n",
      "Test loss at epoch 70:  0.2765417\n",
      "Test Acc at epoch 70:  0.9052438\n",
      "Train loss at epoch 80:  0.13667203\n",
      "Train Acc at epoch 80:  0.9089128\n",
      "Eval loss at epoch 80:  0.2646098\n",
      "Eval Acc at epoch 80:  0.9091039\n",
      "Test loss at epoch 80:  0.28555802\n",
      "Test Acc at epoch 80:  0.9091043\n",
      "Train loss at epoch 90:  0.11797514\n",
      "Train Acc at epoch 90:  0.91255933\n",
      "Eval loss at epoch 90:  0.27106896\n",
      "Eval Acc at epoch 90:  0.9127431\n",
      "Test loss at epoch 90:  0.29272515\n",
      "Test Acc at epoch 90:  0.91273814\n",
      "Train loss at epoch 100:  0.095309675\n",
      "Train Acc at epoch 100:  0.91602397\n",
      "Eval loss at epoch 100:  0.27454525\n",
      "Eval Acc at epoch 100:  0.91620827\n",
      "Test loss at epoch 100:  0.29300097\n",
      "Test Acc at epoch 100:  0.91619813\n",
      "\n",
      "Total time taken (in seconds): 9767.78\n"
     ]
    }
   ],
   "source": [
    "# Specify the path where you want to save/restore the trained variables.\n",
    "checkpoint_directory = 'models_checkpoints/mnist/'\n",
    "\n",
    "# Use the GPU if available.\n",
    "device = 'gpu:0'\n",
    "\n",
    "for i in range(3):\n",
    "    time_start = time.time()\n",
    "    np.random.seed(i)\n",
    "    tf.random.set_seed(i)\n",
    "    # Define optimizer.\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4)\n",
    "\n",
    "    # Instantiate model. This doesn't initialize the variables yet.\n",
    "    model = ImageRecognitionCNN(num_classes=10, device=device, \n",
    "                                  checkpoint_directory=checkpoint_directory)\n",
    "\n",
    "    #model = ImageRecognitionCNN(num_classes=7, device=device)\n",
    "    # Train model\n",
    "    model.fit_fc(train_dataset, val_dataset, test_dataset, optimizer, num_epochs=100, \n",
    "              early_stopping_rounds=100, verbose=10, train_from_scratch=True)\n",
    "    time_taken = time.time() - time_start \n",
    "    print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2f60d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
