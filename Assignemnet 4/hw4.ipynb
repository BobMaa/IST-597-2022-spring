{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a8f0e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43acd7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data() # Load MNIST or FMNIST\n",
    "assert X_train.shape == (60000, 28, 28)\n",
    "assert X_test.shape == (10000, 28, 28)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)\n",
    "\n",
    "\n",
    "# Display randomly selected data\n",
    "indices = list(np.random.randint(X_train.shape[0],size=3))\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.imshow(X_train[indices[i]].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Index {} Class {}\".format(indices[i], y_train[indices[i]]))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb371410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set is 50000 samples\n",
      "every train example is 28 by 28\n",
      "size of validation set is 10000 samples\n",
      "every validation example is 28 by 28\n",
      "size of training set is 50000 samples\n",
      "every train example has 784 features\n",
      "size of validation set is 10000 samples\n",
      "every validation example has 784 features\n"
     ]
    }
   ],
   "source": [
    "# Split train dataset into train and validation\n",
    "X_val = X_train[50000:60000]\n",
    "X_train = X_train[0:50000]\n",
    "y_val = y_train[50000:60000]\n",
    "y_train = y_train[0:50000]\n",
    "\n",
    "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
    "print(\"every train example is\", str(X_train.shape[1]), \"by\", str(X_train.shape[2]))\n",
    "\n",
    "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
    "print(\"every validation example is\", str(X_val.shape[1]), \"by\", str(X_val.shape[2]))\n",
    "\n",
    "X_train = X_train.reshape(50000, 28*28)\n",
    "X_val = X_val.reshape(10000, 28*28)\n",
    "X_test = X_test.reshape(10000, 28*28)\n",
    "\n",
    "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
    "print(\"every train example has\", str(X_train.shape[1]), \"features\")\n",
    "\n",
    "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
    "print(\"every validation example has\", str(X_val.shape[1]), \"features\")\n",
    "\n",
    "# Split dataset into batches\n",
    "#train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16)\n",
    "#test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a533ac72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalize Data\n",
    "\n",
    "X_train = X_train/255\n",
    "X_val = X_val/255\n",
    "X_test = X_test/255\n",
    "# X_train[0]\n",
    "np.max(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93e9afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([10000    10], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "size_input = X_train.shape[1]\n",
    "size_hidden1 = 128\n",
    "size_hidden2 = 128\n",
    "size_hidden3 = 128\n",
    "size_output = len(set(y_train))\n",
    "\n",
    "number_of_train_examples = X_train.shape[0]\n",
    "number_of_test_examples = X_test.shape[0]\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10) # Other function is tf.one_hot(y_train,depth=10)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "print(tf.shape(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5308cec",
   "metadata": {},
   "source": [
    "# MLP with pre-activation BN -- g(BN(f(x)))# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f67f6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class to build mlp model\n",
    "class BNMLP(object):\n",
    "    def __init__(self, size_input, size_hidden1, size_hidden2,size_output, device=None,\\\n",
    "                 regularizer=None, R_lambda = 1e-4, drop_prob=0):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden1: int, size of the 1st hidden layer\n",
    "        size_hidden2: int, size of the 2nd hidden layer\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
    "        \"\"\"\n",
    "        self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
    "        size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
    "        self.regularizer, self.R_lambda, self.drop_prob = regularizer, R_lambda, drop_prob\n",
    "\n",
    "        # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
    "        # Initialize biases for hidden layer\n",
    "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
    "\n",
    "        # Initialize weights between input layer and 1st hidden layer\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
    "        # Initialize biases for hidden layer\n",
    "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
    "\n",
    "        # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
    "        # Initialize biases for hidden layer\n",
    "        self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
    "\n",
    "         # Initialize weights between 2nd hidden layer and output layer\n",
    "        self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
    "        # Initialize biases for output layer\n",
    "        self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
    "             \n",
    "        # Initialize BN scale parameter in the input layer\n",
    "        self.gamma0 = tf.Variable(tf.random.normal([1,self.size_input],stddev=0.1))\n",
    "        self.beta0 = tf.Variable(tf.zeros([1, self.size_input]))\n",
    "        self.batch_mean0 = tf.Variable(tf.zeros([1, self.size_input]))\n",
    "        self.batch_var0 = tf.Variable(tf.ones([1, self.size_input]))\n",
    "        \n",
    "        # Initialize BN scale parameter in the hidden layer 1\n",
    "        self.gamma1 = tf.Variable(tf.random.normal([1,self.size_hidden1],stddev=0.1))\n",
    "        self.beta1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
    "        self.batch_mean1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
    "        self.batch_var1 = tf.Variable(tf.ones([1, self.size_hidden1]))\n",
    "        \n",
    "        # Initialize BN scale parameter in the hidden layer 2\n",
    "        self.gamma2 = tf.Variable(tf.random.normal([1,self.size_hidden2],stddev=0.1))\n",
    "        self.beta2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
    "        self.batch_mean2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
    "        self.batch_var2 = tf.Variable(tf.ones([1, self.size_hidden2]))\n",
    "        \n",
    "        # Initialize BN scale parameter in the hidden layer 3\n",
    "        self.gamma3 = tf.Variable(tf.random.normal([1,self.size_hidden3],stddev=0.1))\n",
    "        self.beta3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
    "        self.batch_mean3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
    "        self.batch_var3 = tf.Variable(tf.ones([1, self.size_hidden3]))\n",
    "        \n",
    "        self.momentum=0.9\n",
    "\n",
    "        # Define variables to be updated during backpropagation\n",
    "        self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4,\\\n",
    "                         self.gamma0, self.gamma1, self.gamma2, self.gamma3, self.beta0, self.beta1, self.beta2, self.beta3]\n",
    "        \n",
    "    def forward(self, X, mode=0):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        X: Tensor, inputs\n",
    "        mode=0: training, mode=1: inference\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "\n",
    "        return self.y\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        '''  \n",
    "        #cross entropy loss for classifation mission\n",
    "        return tf.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits = True)\n",
    "        #return tf.reduce_sum(-tf.math.log(tf.boolean_mask(y_pred, tf.one_hot(y_true, depth=y_pred.shape[-1]))))/y_pred.shape[0]\n",
    "\n",
    "    def backward(self, X_train, y_train, hyperparams, method='sgd'):\n",
    "        \"\"\"\n",
    "        backward pass\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train, mode=0)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "            \n",
    "            num_layer = 3\n",
    "            if not self.regularizer:\n",
    "                current_loss = self.loss(predicted, y_train)\n",
    "            elif self.regularizer == 'l2':\n",
    "                #flatten shape\n",
    "                w = tf.concat([tf.reshape(w,[-1]) for w in self.variables[:num_layer]],0)\n",
    "                current_loss  += self.R_lambda * tf.nn.l2_loss(w)\n",
    "            elif self.regularizer == 'l1':\n",
    "                #flatten shape\n",
    "                w = tf.concat([tf.reshape(w,[-1]) for w in self.variables[:num_layer]],0)\n",
    "                current_loss  += self.R_lambda * tf.nn.l1_loss(w)\n",
    "            \n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "        \n",
    "        if method == 'sgd':\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate = hyperparams['lr'])\n",
    "            optimizer.apply_gradients(zip(grads, self.variables))\n",
    "\n",
    "    def compute_output(self, X, mode=0):\n",
    "        \"\"\"\n",
    "        Custom method to obtain output tensor during forward pass\n",
    "        \"\"\"\n",
    "        epsilon=1e-6\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        # Cast X to float32\n",
    "        \n",
    "        \n",
    "        ##BN in input layer\n",
    "        #training mode\n",
    "        if mode==0:\n",
    "            # compute the sample_mean and sample_var for current batch\n",
    "            sample_mean0 = tf.math.reduce_mean(X_tf, axis=0)\n",
    "            sample_var0 = tf.math.reduce_variance(X_tf, axis=0)\n",
    "            now_normalize0 = (X_tf - sample_mean0) / tf.sqrt(sample_var0+epsilon) # should be broadcastable..\n",
    "            X_tf = self.gamma0 * now_normalize0 + self.beta0\n",
    "            #update batch mean and variance\n",
    "            self.batch_mean0 = self.momentum * self.batch_mean0 + (1.0-self.momentum) * sample_mean0\n",
    "            self.batch_var0 = self.momentum * self.batch_var0 + (1.0-self.momentum)*sample_var0\n",
    "        else:\n",
    "            X_tf = self.gamma0 * (X_tf - self.batch_mean0) / tf.sqrt(self.batch_var0+epsilon) + self.beta0\n",
    "        \n",
    "\n",
    "        # Compute values in hidden layers\n",
    "        z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "        \n",
    "        #BN in hidden layer 1\n",
    "        #training mode\n",
    "        if mode==0:\n",
    "            # compute the sample_mean and sample_var for current batch\n",
    "            sample_mean1 = tf.math.reduce_mean(z1, axis=0)\n",
    "            sample_var1 = tf.math.reduce_variance(z1, axis=0)\n",
    "            now_normalize1 = (z1 - sample_mean1) / tf.sqrt(sample_var1+epsilon) # should be broadcastable..\n",
    "            z1 = self.gamma1 * now_normalize1 + self.beta1\n",
    "            #update batch mean and variance\n",
    "            self.batch_mean1 = self.momentum * self.batch_mean1 + (1.0-self.momentum) * sample_mean1\n",
    "            self.batch_var1 = self.momentum * self.batch_var1 + (1.0-self.momentum)*sample_var1\n",
    "        else:\n",
    "            z1 = self.gamma1 * (z1 - self.batch_mean1) / tf.sqrt(self.batch_var1+epsilon) + self.beta1\n",
    "            \n",
    "        h1 = tf.nn.relu(z1)\n",
    "\n",
    "        z2 = tf.matmul(h1, self.W2) + self.b2\n",
    "        \n",
    "        #BN in hidden layer 2\n",
    "        #training mode\n",
    "        if mode==0:\n",
    "            # compute the sample_mean and sample_var for current batch\n",
    "            sample_mean2 = tf.math.reduce_mean(z2, axis=0)\n",
    "            sample_var2 = tf.math.reduce_variance(z2, axis=0)\n",
    "            now_normalize2 = (z2 - sample_mean2) / tf.sqrt(sample_var2+epsilon) # should be broadcastable..\n",
    "            z2 = self.gamma2 * now_normalize2 + self.beta2\n",
    "            #update batch mean and variance\n",
    "            self.batch_mean2 = self.momentum * self.batch_mean2 + (1.0-self.momentum) * sample_mean2\n",
    "            self.batch_var2 = self.momentum * self.batch_var2 + (1.0-self.momentum)*sample_var2\n",
    "        else:\n",
    "            z2 = self.gamma2 * (z2 - self.batch_mean2) / tf.sqrt(self.batch_var2+epsilon) + self.beta2\n",
    "        h2 = tf.nn.relu(z2)\n",
    "\n",
    "        z3 = tf.matmul(h2, self.W3) + self.b3\n",
    "        \n",
    "        #BN in hidden layer 3\n",
    "        #training mode\n",
    "        if mode==0:\n",
    "            # compute the sample_mean and sample_var for current batch\n",
    "            sample_mean3 = tf.math.reduce_mean(z3, axis=0)\n",
    "            sample_var3 = tf.math.reduce_variance(z3, axis=0)\n",
    "            now_normalize3 = (z3 - sample_mean3) / tf.sqrt(sample_var3+epsilon) # should be broadcastable..\n",
    "            z3 = self.gamma3 * now_normalize3 + self.beta3\n",
    "            #update batch mean and variance\n",
    "            self.batch_mean3 = self.momentum * self.batch_mean3 + (1.0-self.momentum) * sample_mean3\n",
    "            self.batch_var3 = self.momentum * self.batch_var3 + (1.0-self.momentum)*sample_var3\n",
    "        else:\n",
    "            z3 = self.gamma3 * (z3 - self.batch_mean3) / tf.sqrt(self.batch_var3+epsilon) + self.beta3\n",
    "        h3 = tf.nn.relu(z3)\n",
    "\n",
    "        # Compute output\n",
    "        z4 = tf.matmul(h3, self.W4) + self.b4\n",
    "        output = tf.nn.softmax(z4)\n",
    "\n",
    "        #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
    "        # Second add tf.Softmax(output) and then return this variable\n",
    "        return (output)\n",
    "    \n",
    "    def accuracy(self,y_pred, y_true):\n",
    "        \"\"\"\n",
    "        compute the correct num\n",
    "        y_pred: the probability distribution [[...]] or the predicted label [...]\n",
    "        y_true: the 1-D true label\n",
    "        \"\"\"\n",
    "        #detect if y_pred is a probability distribution \n",
    "        if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "        cmp = tf.cast(y_pred, y_true.dtype) == y_true\n",
    "        \n",
    "        return float(tf.reduce_sum(tf.cast(cmp, tf.int32)))\n",
    "\n",
    "    #  def stderr(self,y_pred):\n",
    "    #     \"\"\"\n",
    "    #      Calculate standard error\n",
    "    #      \"\"\"\n",
    "    #     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "    #     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
    "    #     std_err = std_dev/sqrt(len(y_pred_tf))\n",
    "    #     return std_err \n",
    "\n",
    "\n",
    "    #  def var(self,y_pred):\n",
    "    #     \"\"\"\n",
    "    #      Calculate variance \n",
    "    #      \"\"\"\n",
    "    #     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "    #     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
    "    #     variance = (std_dev**2) # calculate variance\n",
    "    #     return variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9fe3389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 1 - Number of Epoch = 10\n",
      "Train loss:= 2.0055 - Val loss: 2.0110 - Test loss: 2.0069 - Train acc:= 52.14% - Val acc:= 52.20% - Test acc:= 52.17%\n",
      "Number of Simulation = 1 - Number of Epoch = 20\n",
      "Train loss:= 1.7025 - Val loss: 1.7039 - Test loss: 1.7098 - Train acc:= 79.03% - Val acc:= 79.05% - Test acc:= 78.06%\n",
      "Number of Simulation = 1 - Number of Epoch = 30\n",
      "Train loss:= 1.6589 - Val loss: 1.6636 - Test loss: 1.6708 - Train acc:= 81.39% - Val acc:= 80.77% - Test acc:= 79.96%\n",
      "Number of Simulation = 1 - Number of Epoch = 40\n",
      "Train loss:= 1.6453 - Val loss: 1.6526 - Test loss: 1.6586 - Train acc:= 82.34% - Val acc:= 81.40% - Test acc:= 80.84%\n",
      "Number of Simulation = 1 - Number of Epoch = 50\n",
      "Train loss:= 1.6372 - Val loss: 1.6468 - Test loss: 1.6526 - Train acc:= 82.90% - Val acc:= 81.78% - Test acc:= 81.29%\n",
      "Number of Simulation = 1 - Number of Epoch = 60\n",
      "Train loss:= 1.6300 - Val loss: 1.6419 - Test loss: 1.6477 - Train acc:= 83.36% - Val acc:= 81.88% - Test acc:= 81.61%\n",
      "Number of Simulation = 1 - Number of Epoch = 70\n",
      "Train loss:= 1.5986 - Val loss: 1.6150 - Test loss: 1.6219 - Train acc:= 86.99% - Val acc:= 85.30% - Test acc:= 84.42%\n",
      "Number of Simulation = 1 - Number of Epoch = 80\n",
      "Train loss:= 1.5866 - Val loss: 1.6070 - Test loss: 1.6132 - Train acc:= 88.13% - Val acc:= 85.75% - Test acc:= 85.14%\n",
      "Number of Simulation = 1 - Number of Epoch = 90\n",
      "Train loss:= 1.5789 - Val loss: 1.6018 - Test loss: 1.6082 - Train acc:= 88.77% - Val acc:= 86.19% - Test acc:= 85.65%\n",
      "Number of Simulation = 1 - Number of Epoch = 100\n",
      "Train loss:= 1.5732 - Val loss: 1.5990 - Test loss: 1.6053 - Train acc:= 89.32% - Val acc:= 86.55% - Test acc:= 85.83%\n",
      "\n",
      "Total time taken (in seconds): 1828.89\n",
      "Number of Simulation = 2 - Number of Epoch = 10\n",
      "Train loss:= 2.0383 - Val loss: 2.0391 - Test loss: 2.0392 - Train acc:= 61.66% - Val acc:= 61.44% - Test acc:= 61.76%\n",
      "Number of Simulation = 2 - Number of Epoch = 20\n",
      "Train loss:= 1.6977 - Val loss: 1.7014 - Test loss: 1.7060 - Train acc:= 79.15% - Val acc:= 78.95% - Test acc:= 78.30%\n",
      "Number of Simulation = 2 - Number of Epoch = 30\n",
      "Train loss:= 1.6548 - Val loss: 1.6612 - Test loss: 1.6671 - Train acc:= 81.73% - Val acc:= 80.94% - Test acc:= 80.42%\n",
      "Number of Simulation = 2 - Number of Epoch = 40\n",
      "Train loss:= 1.6412 - Val loss: 1.6501 - Test loss: 1.6561 - Train acc:= 82.69% - Val acc:= 81.74% - Test acc:= 81.19%\n",
      "Number of Simulation = 2 - Number of Epoch = 50\n",
      "Train loss:= 1.6341 - Val loss: 1.6450 - Test loss: 1.6514 - Train acc:= 83.19% - Val acc:= 82.12% - Test acc:= 81.39%\n",
      "Number of Simulation = 2 - Number of Epoch = 60\n",
      "Train loss:= 1.6288 - Val loss: 1.6413 - Test loss: 1.6481 - Train acc:= 83.66% - Val acc:= 82.36% - Test acc:= 81.64%\n",
      "Number of Simulation = 2 - Number of Epoch = 70\n",
      "Train loss:= 1.6251 - Val loss: 1.6392 - Test loss: 1.6460 - Train acc:= 84.00% - Val acc:= 82.43% - Test acc:= 81.72%\n",
      "Number of Simulation = 2 - Number of Epoch = 80\n",
      "Train loss:= 1.6208 - Val loss: 1.6366 - Test loss: 1.6437 - Train acc:= 84.33% - Val acc:= 82.65% - Test acc:= 81.87%\n",
      "Number of Simulation = 2 - Number of Epoch = 90\n",
      "Train loss:= 1.6143 - Val loss: 1.6319 - Test loss: 1.6389 - Train acc:= 84.84% - Val acc:= 83.05% - Test acc:= 82.29%\n",
      "Number of Simulation = 2 - Number of Epoch = 100\n",
      "Train loss:= 1.5832 - Val loss: 1.6049 - Test loss: 1.6122 - Train acc:= 88.49% - Val acc:= 85.99% - Test acc:= 85.27%\n",
      "\n",
      "Total time taken (in seconds): 3914.07\n",
      "Number of Simulation = 3 - Number of Epoch = 10\n",
      "Train loss:= 2.0318 - Val loss: 2.0331 - Test loss: 2.0334 - Train acc:= 52.07% - Val acc:= 51.69% - Test acc:= 51.48%\n",
      "Number of Simulation = 3 - Number of Epoch = 20\n",
      "Train loss:= 1.7141 - Val loss: 1.7177 - Test loss: 1.7197 - Train acc:= 78.56% - Val acc:= 78.59% - Test acc:= 78.06%\n",
      "Number of Simulation = 3 - Number of Epoch = 30\n",
      "Train loss:= 1.6558 - Val loss: 1.6605 - Test loss: 1.6674 - Train acc:= 81.71% - Val acc:= 81.33% - Test acc:= 80.47%\n",
      "Number of Simulation = 3 - Number of Epoch = 40\n",
      "Train loss:= 1.6422 - Val loss: 1.6495 - Test loss: 1.6562 - Train acc:= 82.65% - Val acc:= 81.85% - Test acc:= 81.20%\n",
      "Number of Simulation = 3 - Number of Epoch = 50\n",
      "Train loss:= 1.6348 - Val loss: 1.6438 - Test loss: 1.6513 - Train acc:= 83.16% - Val acc:= 82.30% - Test acc:= 81.46%\n",
      "Number of Simulation = 3 - Number of Epoch = 60\n",
      "Train loss:= 1.6294 - Val loss: 1.6399 - Test loss: 1.6472 - Train acc:= 83.63% - Val acc:= 82.46% - Test acc:= 81.81%\n",
      "Number of Simulation = 3 - Number of Epoch = 70\n",
      "Train loss:= 1.6245 - Val loss: 1.6363 - Test loss: 1.6443 - Train acc:= 84.01% - Val acc:= 82.81% - Test acc:= 82.03%\n",
      "Number of Simulation = 3 - Number of Epoch = 80\n",
      "Train loss:= 1.6100 - Val loss: 1.6245 - Test loss: 1.6321 - Train acc:= 86.54% - Val acc:= 84.89% - Test acc:= 84.16%\n",
      "Number of Simulation = 3 - Number of Epoch = 90\n",
      "Train loss:= 1.5868 - Val loss: 1.6054 - Test loss: 1.6133 - Train acc:= 88.11% - Val acc:= 86.10% - Test acc:= 85.27%\n",
      "Number of Simulation = 3 - Number of Epoch = 100\n",
      "Train loss:= 1.5789 - Val loss: 1.6001 - Test loss: 1.6085 - Train acc:= 88.79% - Val acc:= 86.58% - Test acc:= 85.69%\n",
      "\n",
      "Total time taken (in seconds): 1686.74\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set number of simulations and epochs\n",
    "NUM_SIM = 3\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "for num_sim in range(NUM_SIM):\n",
    "    np.random.seed(num_sim)\n",
    "    tf.random.set_seed(num_sim)\n",
    "    '''\n",
    "    Initialize model using GPU or load an exitsing MLP\n",
    "    '''\n",
    "    BN_MLP = BNMLP(size_input, size_hidden1, size_hidden2, size_output, device='GPU',\\\n",
    "                regularizer=None, R_lambda = 1e-4, drop_prob=0.)\n",
    "\n",
    "    time_start = time.time()\n",
    "    hyperparams = {'t':1, 'lr':1e-4}\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*num_sim).batch(128)\n",
    "\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = BN_MLP.forward(inputs, mode = 0)\n",
    "\n",
    "            #use SGD to train the model\n",
    "            BN_MLP.backward(inputs, outputs, hyperparams,'sgd')\n",
    "            hyperparams['t'] += 1\n",
    "        \n",
    "        if (epoch + 1)%10 == 0:\n",
    "            #compute the result for the current epoch\n",
    "            logits = BN_MLP.forward(X_train, mode = 1)\n",
    "            train_loss = np.sum(BN_MLP.loss(logits, y_train))/len(y_train)\n",
    "            train_acc = BN_MLP.accuracy(logits,y_train)/len(y_train)\n",
    "\n",
    "            logits = BN_MLP.forward(X_val, mode = 1)\n",
    "            val_loss = np.sum(BN_MLP.loss(logits, y_val))/len(y_val)\n",
    "            val_acc = BN_MLP.accuracy(logits,y_val)/len(y_val)\n",
    "            \n",
    "            logits = BN_MLP.forward(X_test, mode = 1)\n",
    "            test_loss = np.sum(BN_MLP.loss(logits, y_test))/len(y_test)\n",
    "            test_acc = BN_MLP.accuracy(logits,y_test)/len(y_test)\n",
    "            \n",
    "            print('Number of Simulation = {} - Number of Epoch = {}'.format(num_sim+1, epoch + 1))\n",
    "            print('Train loss:= {:.4f} - Val loss: {:.4f} - Test loss: {:.4f} - Train acc:= {:.2%} - Val acc:= {:.2%} - Test acc:= {:.2%}'\\\n",
    "                  .format(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc))\n",
    "            \n",
    "    time_taken = time.time() - time_start \n",
    "    print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc033f4",
   "metadata": {},
   "source": [
    "# MLP with post-activation BN -- BN(g(f(x)))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "544b49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class to build mlp model\n",
    "class BNMLP(object):\n",
    "    def __init__(self, size_input, size_hidden1, size_hidden2,size_output, device=None,\\\n",
    "                 regularizer=None, R_lambda = 1e-4, drop_prob=0):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden1: int, size of the 1st hidden layer\n",
    "        size_hidden2: int, size of the 2nd hidden layer\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
    "        \"\"\"\n",
    "        self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
    "        size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
    "        self.regularizer, self.R_lambda, self.drop_prob = regularizer, R_lambda, drop_prob\n",
    "\n",
    "        # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
    "        # Initialize biases for hidden layer\n",
    "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
    "\n",
    "        # Initialize weights between input layer and 1st hidden layer\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
    "        # Initialize biases for hidden layer\n",
    "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
    "\n",
    "        # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
    "        # Initialize biases for hidden layer\n",
    "        self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
    "\n",
    "         # Initialize weights between 2nd hidden layer and output layer\n",
    "        self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
    "        # Initialize biases for output layer\n",
    "        self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
    "             \n",
    "        # Initialize BN scale parameter in the input layer\n",
    "        self.gamma0 = tf.Variable(tf.random.normal([1,self.size_input],stddev=0.1))\n",
    "        self.beta0 = tf.Variable(tf.zeros([1, self.size_input]))\n",
    "        self.batch_mean0 = tf.Variable(tf.zeros([1, self.size_input]))\n",
    "        self.batch_var0 = tf.Variable(tf.ones([1, self.size_input]))\n",
    "        \n",
    "        # Initialize BN scale parameter in the hidden layer 1\n",
    "        self.gamma1 = tf.Variable(tf.random.normal([1,self.size_hidden1],stddev=0.1))\n",
    "        self.beta1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
    "        self.batch_mean1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
    "        self.batch_var1 = tf.Variable(tf.ones([1, self.size_hidden1]))\n",
    "        \n",
    "        # Initialize BN scale parameter in the hidden layer 2\n",
    "        self.gamma2 = tf.Variable(tf.random.normal([1,self.size_hidden2],stddev=0.1))\n",
    "        self.beta2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
    "        self.batch_mean2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
    "        self.batch_var2 = tf.Variable(tf.ones([1, self.size_hidden2]))\n",
    "        \n",
    "        # Initialize BN scale parameter in the hidden layer 3\n",
    "        self.gamma3 = tf.Variable(tf.random.normal([1,self.size_hidden3],stddev=0.1))\n",
    "        self.beta3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
    "        self.batch_mean3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
    "        self.batch_var3 = tf.Variable(tf.ones([1, self.size_hidden3]))\n",
    "        \n",
    "        self.momentum=0.9\n",
    "\n",
    "        # Define variables to be updated during backpropagation\n",
    "        self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4,\\\n",
    "                         self.gamma0, self.gamma1, self.gamma2, self.gamma3, self.beta0, self.beta1, self.beta2, self.beta3]\n",
    "        \n",
    "    def forward(self, X, mode=0):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        X: Tensor, inputs\n",
    "        mode=0: training, mode=1: inference\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "\n",
    "        return self.y\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        '''  \n",
    "        #cross entropy loss for classifation mission\n",
    "        return tf.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits = True)\n",
    "        #return tf.reduce_sum(-tf.math.log(tf.boolean_mask(y_pred, tf.one_hot(y_true, depth=y_pred.shape[-1]))))/y_pred.shape[0]\n",
    "\n",
    "    def backward(self, X_train, y_train, hyperparams, method='sgd'):\n",
    "        \"\"\"\n",
    "        backward pass\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train, mode=0)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "            \n",
    "            num_layer = 3\n",
    "            if not self.regularizer:\n",
    "                current_loss = self.loss(predicted, y_train)\n",
    "            elif self.regularizer == 'l2':\n",
    "                #flatten shape\n",
    "                w = tf.concat([tf.reshape(w,[-1]) for w in self.variables[:num_layer]],0)\n",
    "                current_loss  += self.R_lambda * tf.nn.l2_loss(w)\n",
    "            elif self.regularizer == 'l1':\n",
    "                #flatten shape\n",
    "                w = tf.concat([tf.reshape(w,[-1]) for w in self.variables[:num_layer]],0)\n",
    "                current_loss  += self.R_lambda * tf.nn.l1_loss(w)\n",
    "            \n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "        \n",
    "        if method == 'sgd':\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate = hyperparams['lr'])\n",
    "            optimizer.apply_gradients(zip(grads, self.variables))\n",
    "\n",
    "    def compute_output(self, X, mode=0):\n",
    "        \"\"\"\n",
    "        Custom method to obtain output tensor during forward pass\n",
    "        \"\"\"\n",
    "        epsilon=1e-6\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        # Cast X to float32\n",
    "        \n",
    "        \n",
    "        ##BN in input layer\n",
    "        #training mode\n",
    "        if mode==0:\n",
    "            # compute the sample_mean and sample_var for current batch\n",
    "            sample_mean0 = tf.math.reduce_mean(X_tf, axis=0)\n",
    "            sample_var0 = tf.math.reduce_variance(X_tf, axis=0)\n",
    "            now_normalize0 = (X_tf - sample_mean0) / tf.sqrt(sample_var0+epsilon) # should be broadcastable..\n",
    "            X_tf = self.gamma0 * now_normalize0 + self.beta0\n",
    "            #update batch mean and variance\n",
    "            self.batch_mean0 = self.momentum * self.batch_mean0 + (1.0-self.momentum) * sample_mean0\n",
    "            self.batch_var0 = self.momentum * self.batch_var0 + (1.0-self.momentum)*sample_var0\n",
    "        else:\n",
    "            X_tf = self.gamma0 * (X_tf - self.batch_mean0) / tf.sqrt(self.batch_var0+epsilon) + self.beta0\n",
    "        \n",
    "\n",
    "        # Compute values in hidden layers\n",
    "        z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "        \n",
    "        h1 = tf.nn.relu(z1)\n",
    "        \n",
    "        #BN in hidden layer 1\n",
    "        #training mode\n",
    "        if mode==0:\n",
    "            # compute the sample_mean and sample_var for current batch\n",
    "            sample_mean1 = tf.math.reduce_mean(h1, axis=0)\n",
    "            sample_var1 = tf.math.reduce_variance(h1, axis=0)\n",
    "            now_normalize1 = (h1 - sample_mean1) / tf.sqrt(sample_var1+epsilon) # should be broadcastable..\n",
    "            h1 = self.gamma1 * now_normalize1 + self.beta1\n",
    "            #update batch mean and variance\n",
    "            self.batch_mean1 = self.momentum * self.batch_mean1 + (1.0-self.momentum) * sample_mean1\n",
    "            self.batch_var1 = self.momentum * self.batch_var1 + (1.0-self.momentum)*sample_var1\n",
    "        else:\n",
    "            h1 = self.gamma1 * (h1 - self.batch_mean1) / tf.sqrt(self.batch_var1+epsilon) + self.beta1\n",
    "            \n",
    "        \n",
    "\n",
    "        z2 = tf.matmul(h1, self.W2) + self.b2\n",
    "        h2 = tf.nn.relu(z2)\n",
    "        #BN in hidden layer 2\n",
    "        #training mode\n",
    "        if mode==0:\n",
    "            # compute the sample_mean and sample_var for current batch\n",
    "            sample_mean2 = tf.math.reduce_mean(h2, axis=0)\n",
    "            sample_var2 = tf.math.reduce_variance(h2, axis=0)\n",
    "            now_normalize2 = (h2 - sample_mean2) / tf.sqrt(sample_var2+epsilon) # should be broadcastable..\n",
    "            h2 = self.gamma2 * now_normalize2 + self.beta2\n",
    "            #update batch mean and variance\n",
    "            self.batch_mean2 = self.momentum * self.batch_mean2 + (1.0-self.momentum) * sample_mean2\n",
    "            self.batch_var2 = self.momentum * self.batch_var2 + (1.0-self.momentum)*sample_var2\n",
    "        else:\n",
    "            h2 = self.gamma2 * (h2 - self.batch_mean2) / tf.sqrt(self.batch_var2+epsilon) + self.beta2\n",
    "        \n",
    "\n",
    "        z3 = tf.matmul(h2, self.W3) + self.b3\n",
    "        h3 = tf.nn.relu(z3)\n",
    "        #BN in hidden layer 3\n",
    "        #training mode\n",
    "        if mode==0:\n",
    "            # compute the sample_mean and sample_var for current batch\n",
    "            sample_mean3 = tf.math.reduce_mean(h3, axis=0)\n",
    "            sample_var3 = tf.math.reduce_variance(h3, axis=0)\n",
    "            now_normalize3 = (h3 - sample_mean3) / tf.sqrt(sample_var3+epsilon) # should be broadcastable..\n",
    "            h3 = self.gamma3 * now_normalize3 + self.beta3\n",
    "            #update batch mean and variance\n",
    "            self.batch_mean3 = self.momentum * self.batch_mean3 + (1.0-self.momentum) * sample_mean3\n",
    "            self.batch_var3 = self.momentum * self.batch_var3 + (1.0-self.momentum)*sample_var3\n",
    "        else:\n",
    "            h3 = self.gamma3 * (h3 - self.batch_mean3) / tf.sqrt(self.batch_var3+epsilon) + self.beta3\n",
    "        \n",
    "\n",
    "        # Compute output\n",
    "        z4 = tf.matmul(h3, self.W4) + self.b4\n",
    "        output = tf.nn.softmax(z4)\n",
    "\n",
    "        #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
    "        # Second add tf.Softmax(output) and then return this variable\n",
    "        return (output)\n",
    "    \n",
    "    def accuracy(self,y_pred, y_true):\n",
    "        \"\"\"\n",
    "        compute the correct num\n",
    "        y_pred: the probability distribution [[...]] or the predicted label [...]\n",
    "        y_true: the 1-D true label\n",
    "        \"\"\"\n",
    "        #detect if y_pred is a probability distribution \n",
    "        if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "        cmp = tf.cast(y_pred, y_true.dtype) == y_true\n",
    "        \n",
    "        return float(tf.reduce_sum(tf.cast(cmp, tf.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09a93e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 1 - Number of Epoch = 10\n",
      "Train loss:= 1.7314 - Val loss: 1.7314 - Test loss: 1.7367 - Train acc:= 77.23% - Val acc:= 77.42% - Test acc:= 76.84%\n",
      "Number of Simulation = 1 - Number of Epoch = 20\n",
      "Train loss:= 1.6588 - Val loss: 1.6613 - Test loss: 1.6689 - Train acc:= 81.15% - Val acc:= 80.96% - Test acc:= 80.27%\n",
      "Number of Simulation = 1 - Number of Epoch = 30\n",
      "Train loss:= 1.6165 - Val loss: 1.6263 - Test loss: 1.6335 - Train acc:= 85.46% - Val acc:= 84.37% - Test acc:= 83.49%\n",
      "Number of Simulation = 1 - Number of Epoch = 40\n",
      "Train loss:= 1.5973 - Val loss: 1.6127 - Test loss: 1.6203 - Train acc:= 87.04% - Val acc:= 85.28% - Test acc:= 84.50%\n",
      "Number of Simulation = 1 - Number of Epoch = 50\n",
      "Train loss:= 1.5872 - Val loss: 1.6060 - Test loss: 1.6124 - Train acc:= 87.90% - Val acc:= 85.78% - Test acc:= 85.25%\n",
      "Number of Simulation = 1 - Number of Epoch = 60\n",
      "Train loss:= 1.5797 - Val loss: 1.6013 - Test loss: 1.6089 - Train acc:= 88.60% - Val acc:= 86.26% - Test acc:= 85.33%\n",
      "Number of Simulation = 1 - Number of Epoch = 70\n",
      "Train loss:= 1.5749 - Val loss: 1.5985 - Test loss: 1.6065 - Train acc:= 89.03% - Val acc:= 86.56% - Test acc:= 85.61%\n",
      "Number of Simulation = 1 - Number of Epoch = 80\n",
      "Train loss:= 1.5710 - Val loss: 1.5971 - Test loss: 1.6054 - Train acc:= 89.34% - Val acc:= 86.55% - Test acc:= 85.65%\n",
      "Number of Simulation = 1 - Number of Epoch = 90\n",
      "Train loss:= 1.5685 - Val loss: 1.5969 - Test loss: 1.6042 - Train acc:= 89.52% - Val acc:= 86.54% - Test acc:= 85.84%\n",
      "Number of Simulation = 1 - Number of Epoch = 100\n",
      "Train loss:= 1.5658 - Val loss: 1.5955 - Test loss: 1.6036 - Train acc:= 89.74% - Val acc:= 86.74% - Test acc:= 85.85%\n",
      "\n",
      "Total time taken (in seconds): 2390.07\n",
      "Number of Simulation = 2 - Number of Epoch = 10\n",
      "Train loss:= 1.7179 - Val loss: 1.7195 - Test loss: 1.7234 - Train acc:= 77.88% - Val acc:= 77.83% - Test acc:= 77.16%\n",
      "Number of Simulation = 2 - Number of Epoch = 20\n",
      "Train loss:= 1.6487 - Val loss: 1.6550 - Test loss: 1.6608 - Train acc:= 83.18% - Val acc:= 82.48% - Test acc:= 81.88%\n",
      "Number of Simulation = 2 - Number of Epoch = 30\n",
      "Train loss:= 1.6074 - Val loss: 1.6199 - Test loss: 1.6263 - Train acc:= 86.16% - Val acc:= 84.82% - Test acc:= 84.24%\n",
      "Number of Simulation = 2 - Number of Epoch = 40\n",
      "Train loss:= 1.5903 - Val loss: 1.6064 - Test loss: 1.6144 - Train acc:= 87.70% - Val acc:= 85.97% - Test acc:= 85.10%\n",
      "Number of Simulation = 2 - Number of Epoch = 50\n",
      "Train loss:= 1.5814 - Val loss: 1.6009 - Test loss: 1.6082 - Train acc:= 88.45% - Val acc:= 86.42% - Test acc:= 85.57%\n",
      "Number of Simulation = 2 - Number of Epoch = 60\n",
      "Train loss:= 1.5739 - Val loss: 1.5961 - Test loss: 1.6039 - Train acc:= 89.13% - Val acc:= 86.84% - Test acc:= 86.00%\n",
      "Number of Simulation = 2 - Number of Epoch = 70\n",
      "Train loss:= 1.5697 - Val loss: 1.5942 - Test loss: 1.6029 - Train acc:= 89.53% - Val acc:= 86.79% - Test acc:= 86.07%\n",
      "Number of Simulation = 2 - Number of Epoch = 80\n",
      "Train loss:= 1.5645 - Val loss: 1.5915 - Test loss: 1.6000 - Train acc:= 90.01% - Val acc:= 87.18% - Test acc:= 86.17%\n",
      "Number of Simulation = 2 - Number of Epoch = 90\n",
      "Train loss:= 1.5607 - Val loss: 1.5901 - Test loss: 1.5993 - Train acc:= 90.34% - Val acc:= 87.54% - Test acc:= 86.45%\n",
      "Number of Simulation = 2 - Number of Epoch = 100\n",
      "Train loss:= 1.5590 - Val loss: 1.5907 - Test loss: 1.5996 - Train acc:= 90.48% - Val acc:= 87.18% - Test acc:= 86.23%\n",
      "\n",
      "Total time taken (in seconds): 3730.12\n",
      "Number of Simulation = 3 - Number of Epoch = 10\n",
      "Train loss:= 1.7715 - Val loss: 1.7752 - Test loss: 1.7759 - Train acc:= 74.58% - Val acc:= 74.38% - Test acc:= 73.96%\n",
      "Number of Simulation = 3 - Number of Epoch = 20\n",
      "Train loss:= 1.6328 - Val loss: 1.6410 - Test loss: 1.6461 - Train acc:= 84.48% - Val acc:= 83.44% - Test acc:= 83.02%\n",
      "Number of Simulation = 3 - Number of Epoch = 30\n",
      "Train loss:= 1.6008 - Val loss: 1.6140 - Test loss: 1.6247 - Train acc:= 86.91% - Val acc:= 85.48% - Test acc:= 84.15%\n",
      "Number of Simulation = 3 - Number of Epoch = 40\n",
      "Train loss:= 1.5866 - Val loss: 1.6035 - Test loss: 1.6151 - Train acc:= 88.16% - Val acc:= 86.20% - Test acc:= 84.94%\n",
      "Number of Simulation = 3 - Number of Epoch = 50\n",
      "Train loss:= 1.5780 - Val loss: 1.6000 - Test loss: 1.6109 - Train acc:= 88.88% - Val acc:= 86.46% - Test acc:= 85.28%\n",
      "Number of Simulation = 3 - Number of Epoch = 60\n",
      "Train loss:= 1.5708 - Val loss: 1.5957 - Test loss: 1.6068 - Train acc:= 89.51% - Val acc:= 86.86% - Test acc:= 85.66%\n",
      "Number of Simulation = 3 - Number of Epoch = 70\n",
      "Train loss:= 1.5662 - Val loss: 1.5955 - Test loss: 1.6048 - Train acc:= 89.94% - Val acc:= 86.69% - Test acc:= 85.81%\n",
      "Number of Simulation = 3 - Number of Epoch = 80\n",
      "Train loss:= 1.5619 - Val loss: 1.5931 - Test loss: 1.6030 - Train acc:= 90.31% - Val acc:= 86.94% - Test acc:= 85.99%\n",
      "Number of Simulation = 3 - Number of Epoch = 90\n",
      "Train loss:= 1.5581 - Val loss: 1.5914 - Test loss: 1.6023 - Train acc:= 90.62% - Val acc:= 87.12% - Test acc:= 86.02%\n",
      "Number of Simulation = 3 - Number of Epoch = 100\n",
      "Train loss:= 1.5552 - Val loss: 1.5912 - Test loss: 1.6017 - Train acc:= 90.90% - Val acc:= 87.03% - Test acc:= 86.09%\n",
      "\n",
      "Total time taken (in seconds): 3383.93\n"
     ]
    }
   ],
   "source": [
    "# Set number of simulations and epochs\n",
    "NUM_SIM = 3\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "for num_sim in range(NUM_SIM):\n",
    "    np.random.seed(num_sim)\n",
    "    tf.random.set_seed(num_sim)\n",
    "    '''\n",
    "    Initialize model using GPU or load an exitsing MLP\n",
    "    '''\n",
    "    BN_MLP = BNMLP(size_input, size_hidden1, size_hidden2, size_output, device='GPU',\\\n",
    "                regularizer=None, R_lambda = 1e-4, drop_prob=0.)\n",
    "\n",
    "    time_start = time.time()\n",
    "    hyperparams = {'t':1, 'lr':1e-4}\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*num_sim).batch(128)\n",
    "\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = BN_MLP.forward(inputs, mode = 0)\n",
    "\n",
    "            #use SGD to train the model\n",
    "            BN_MLP.backward(inputs, outputs, hyperparams,'sgd')\n",
    "            hyperparams['t'] += 1\n",
    "        \n",
    "        if (epoch + 1)%10 == 0:\n",
    "            #compute the result for the current epoch\n",
    "            logits = BN_MLP.forward(X_train, mode = 1)\n",
    "            train_loss = np.sum(BN_MLP.loss(logits, y_train))/len(y_train)\n",
    "            train_acc = BN_MLP.accuracy(logits,y_train)/len(y_train)\n",
    "\n",
    "            logits = BN_MLP.forward(X_val, mode = 1)\n",
    "            val_loss = np.sum(BN_MLP.loss(logits, y_val))/len(y_val)\n",
    "            val_acc = BN_MLP.accuracy(logits,y_val)/len(y_val)\n",
    "            \n",
    "            logits = BN_MLP.forward(X_test, mode = 1)\n",
    "            test_loss = np.sum(BN_MLP.loss(logits, y_test))/len(y_test)\n",
    "            test_acc = BN_MLP.accuracy(logits,y_test)/len(y_test)\n",
    "            \n",
    "            print('Number of Simulation = {} - Number of Epoch = {}'.format(num_sim+1, epoch + 1))\n",
    "            print('Train loss:= {:.4f} - Val loss: {:.4f} - Test loss: {:.4f} - Train acc:= {:.2%} - Val acc:= {:.2%} - Test acc:= {:.2%}'\\\n",
    "                  .format(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc))\n",
    "            \n",
    "    time_taken = time.time() - time_start \n",
    "    print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f6a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
