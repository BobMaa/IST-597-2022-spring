{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57162d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def12cdd",
   "metadata": {},
   "source": [
    "Set Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8d863b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size: (50000, 784)\n",
      "val_size: (10000, 784)\n",
      "test_size: (10000, 784)\n",
      "train_output_size (50000,)\n",
      "val_output_size: (10000,)\n",
      "test_output_size (10000,)\n",
      "max_val: 1.0\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# load and normalize data\n",
    "#(X_train, y_train), (X_test, y_test) =  tf.keras.datasets.mnist.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "#Normalization\n",
    "X_train = tf.reshape(X_train, (X_train.shape[0],-1))/255\n",
    "X_test = tf.reshape(X_test, (X_test.shape[0],-1))/255\n",
    "\n",
    "#last 10000 training examples for validation\n",
    "X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "print(\"train_size:\", X_train.shape)\n",
    "print(\"val_size:\", X_val.shape)\n",
    "print(\"test_size:\", X_test.shape)\n",
    "print(\"train_output_size\", y_train.shape)\n",
    "print(\"val_output_size:\", y_val.shape)\n",
    "print(\"test_output_size\", y_test.shape)\n",
    "print(\"max_val:\", np.max(X_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87dd71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_input = X_train.shape[1]\n",
    "size_output = len(set(y_train))\n",
    "size_hidden1 = 256\n",
    "size_hidden2 = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b94797",
   "metadata": {},
   "source": [
    "Build MLP using Eager Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a947367",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, size_input, size_hidden1, size_hidden2, size_output, device=None, regularizer=None, R_lambda = 1e-4, drop_prob=0):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden: int, size of hidden layer\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
    "        regularizer: str or None\n",
    "        R_lambda: the parameter for regularizer\n",
    "        drop_prob: 0 to 1\n",
    "        \"\"\"\n",
    "        self.size_input, self.size_hidden1, self.size_hidden2, self.size_output, self.device =\\\n",
    "        size_input, size_hidden1, size_hidden2, size_output, device\n",
    "        \n",
    "        self.regularizer, self.R_lambda, self.drop_prob = regularizer, R_lambda, drop_prob\n",
    "        \n",
    "        # Initialize weights between input layer and hidden layer 1\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) \n",
    "        # Initialize biases for hidden layer 1\n",
    "        self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))# 0 or constant(0.01)\n",
    "\n",
    "        # Initialize weights between hidden layer 1 and hidden layer 2\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
    "        # Initialize biases for hidden layer 2\n",
    "        self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
    "\n",
    "         # Initialize weights between hidden layer 2 and output layer\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output],stddev=0.1))\n",
    "        # Initialize biases for output layer\n",
    "        self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
    "\n",
    "        # Define variables to be updated during backpropagation\n",
    "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
    "        \n",
    "        # Initialize the state of custom optimizer\n",
    "        self.v_W1 = tf.Variable(tf.zeros([self.size_input, self.size_hidden1]))\n",
    "        self.v_b1 = tf.Variable(tf.zeros([1,self.size_hidden1]))\n",
    "        self.u_W1 = tf.Variable(tf.zeros([self.size_input, self.size_hidden1]))\n",
    "        self.u_b1 = tf.Variable(tf.zeros([1,self.size_hidden1]))\n",
    "        self.m_W1 = tf.Variable(tf.zeros([self.size_input, self.size_hidden1]))\n",
    "        self.m_b1 = tf.Variable(tf.zeros([1,self.size_hidden1]))\n",
    "        \n",
    "        self.v_W2 = tf.Variable(tf.zeros([self.size_hidden1, self.size_hidden2]))\n",
    "        self.v_b2 = tf.Variable(tf.zeros([1,self.size_hidden2]))\n",
    "        self.u_W2 = tf.Variable(tf.zeros([self.size_hidden1, self.size_hidden2]))\n",
    "        self.u_b2 = tf.Variable(tf.zeros([1,self.size_hidden2]))\n",
    "        self.m_W2 = tf.Variable(tf.zeros([self.size_hidden1, self.size_hidden2]))\n",
    "        self.m_b2 = tf.Variable(tf.zeros([1,self.size_hidden2]))\n",
    "        \n",
    "        self.v_W3 = tf.Variable(tf.zeros([self.size_hidden2, self.size_output]))\n",
    "        self.v_b3 = tf.Variable(tf.zeros([1,self.size_output]))\n",
    "        self.u_W3 = tf.Variable(tf.zeros([self.size_hidden2, self.size_output]))\n",
    "        self.u_b3 = tf.Variable(tf.zeros([1,self.size_output]))\n",
    "        self.m_W3 = tf.Variable(tf.zeros([self.size_hidden2, self.size_output]))\n",
    "        self.m_b3 = tf.Variable(tf.zeros([1,self.size_output]))\n",
    "        \n",
    "        self.v_state = [self.v_W1,self.v_W2,self.v_W3,self.v_b1,self.v_b2,self.v_b3]\n",
    "        self.u_state = [self.u_W1,self.u_W2,self.u_W3,self.u_b1,self.u_b2,self.u_b3]\n",
    "        self.m_state = [self.m_W1,self.m_W2,self.m_W3,self.m_b1,self.m_b2,self.m_b3]\n",
    "       \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        X: Tensor, inputs\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "\n",
    "        return self.y\n",
    "    \n",
    "    def loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        '''  \n",
    "        #cross entropy loss for classifation mission\n",
    "        return tf.losses.sparse_categorical_crossentropy(y_true,y_pred, from_logits = False)\n",
    "        \n",
    "    def backward(self, X_train, y_train, hyperparams, method='custom'):\n",
    "        \"\"\"\n",
    "        backward pass\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "            \n",
    "            num_layer = 3\n",
    "            if not self.regularizer:\n",
    "                current_loss = self.loss(predicted, y_train)\n",
    "            #l2 norm\n",
    "            elif self.regularizer == 'l2':\n",
    "                w = tf.concat([tf.reshape(w,[-1]) for w in self.variables[:num_layer]],0)#self.variable[:3] -> w1,w2,w3\n",
    "                current_loss  += self.R_lambda * tf.nn.l2_loss(w)\n",
    "            #l1 norm\n",
    "            elif self.regularizer == 'l1':\n",
    "                w = tf.concat([tf.reshape(w,[-1]) for w in self.variables[:num_layer]],0)\n",
    "                current_loss  += self.R_lambda * tf.nn.l1_loss(w)\n",
    "            \n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "        \n",
    "        if method == 'sgd':\n",
    "            optimizer = tf.keras.optimizers.SGD(learning_rate = hyperparams['lr'])\n",
    "            optimizer.apply_gradients(zip(grads, self.variables))\n",
    "        elif method == 'adam':\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=hyperparams['lr'], beta_1=0.9, beta_2=0.999, epsilon=1e-6,amsgrad=False,\\\n",
    "                                                 name='Adam')\n",
    "            optimizer.apply_gradients(zip(grads, self.variables))\n",
    "        elif method == 'RMSprop':\n",
    "            optimizer = tf.keras.optimizers.RMSprop(learning_rate = hyperparams['lr'])\n",
    "            optimizer.apply_gradients(zip(grads, self.variables))\n",
    "        elif method == 'custom':\n",
    "            #Custom optimizer\n",
    "            beta1,beta2,beta3,eps = 0.9,0.999,0.999987,1e-8\n",
    "\n",
    "            for p,m,v,u,grad in zip(self.variables, self.m_state, self.v_state, self.u_state, grads):\n",
    "                m[:].assign(beta1 * m  + (1 - beta1) * grad)\n",
    "                v[:].assign(beta2 * v  + (1 - beta2) * tf.math.square(grad))\n",
    "                u[:].assign(beta3 * u  + (1 - beta3) * tf.math.pow(grad, 3))\n",
    "                m_bias_corr = m / (1 - beta1 ** hyperparams['t'])\n",
    "                v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n",
    "                u_bias_corr = u / (1 - beta2 ** hyperparams['t'])\n",
    "                p[:].assign(p - hyperparams['lr'] * m_bias_corr/ (tf.math.sqrt(v_bias_corr) +\\\n",
    "                                                                  eps*tf.math.sign(u_bias_corr)*tf.math.pow(abs(u_bias_corr),1.0/3.0) + eps))\n",
    "    def compute_output(self,X):\n",
    "        # Cast X to float32\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "            \n",
    "        #set the dropout prob\n",
    "        prob = self.drop_prob\n",
    "\n",
    "        # Compute values in hidden layer 1\n",
    "        what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "        hhat1 = tf.nn.experimental.stateless_dropout(tf.nn.relu(what1), rate = prob, seed = [1,0])/(1-prob)\n",
    "\n",
    "        # Compute values in hidden layer 2\n",
    "        what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
    "        hhat2 = tf.nn.experimental.stateless_dropout(tf.nn.relu(what2), rate = prob, seed = [1,0])/(1-prob)\n",
    "\n",
    "        # Compute output\n",
    "        output = tf.nn.softmax(tf.matmul(hhat2, self.W3) + self.b3)\n",
    "        return output\n",
    "\n",
    "    def accuracy(self,y_pred, y_true):\n",
    "        \"\"\"\n",
    "        compute the correct num\n",
    "        y_pred: the probability distribution [[...]] or the predicted label [...]\n",
    "        y_true: the 1-D true label\n",
    "        \"\"\"\n",
    "        #detect if y_pred is a probability distribution \n",
    "        if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "            y_pred = tf.argmax(y_pred, axis=1)\n",
    "            \n",
    "        cmp = tf.cast(y_pred, y_true.dtype) == y_true\n",
    "        \n",
    "        return float(tf.reduce_sum(tf.cast(cmp, tf.int32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af828bc1",
   "metadata": {},
   "source": [
    "Custom Optimizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd1320a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 1 - Number of Epoch = 10\n",
      "Train loss:= 0.4398 - Val loss: 0.4563 - Test loss: 0.4779 - Train acc:= 84.82% - Val acc:= 83.78% - Test acc:= 82.97%\n",
      "Number of Simulation = 1 - Number of Epoch = 20\n",
      "Train loss:= 0.3850 - Val loss: 0.4082 - Test loss: 0.4315 - Train acc:= 86.65% - Val acc:= 85.61% - Test acc:= 84.57%\n",
      "Number of Simulation = 1 - Number of Epoch = 30\n",
      "Train loss:= 0.3550 - Val loss: 0.3841 - Test loss: 0.4080 - Train acc:= 87.56% - Val acc:= 86.38% - Test acc:= 85.61%\n",
      "Number of Simulation = 1 - Number of Epoch = 40\n",
      "Train loss:= 0.3336 - Val loss: 0.3683 - Test loss: 0.3926 - Train acc:= 88.35% - Val acc:= 86.80% - Test acc:= 86.08%\n",
      "Number of Simulation = 1 - Number of Epoch = 50\n",
      "Train loss:= 0.3168 - Val loss: 0.3565 - Test loss: 0.3815 - Train acc:= 88.83% - Val acc:= 87.15% - Test acc:= 86.42%\n",
      "Number of Simulation = 1 - Number of Epoch = 60\n",
      "Train loss:= 0.3029 - Val loss: 0.3475 - Test loss: 0.3729 - Train acc:= 89.32% - Val acc:= 87.28% - Test acc:= 86.77%\n",
      "Number of Simulation = 1 - Number of Epoch = 70\n",
      "Train loss:= 0.2908 - Val loss: 0.3403 - Test loss: 0.3662 - Train acc:= 89.69% - Val acc:= 87.51% - Test acc:= 87.04%\n",
      "Number of Simulation = 1 - Number of Epoch = 80\n",
      "Train loss:= 0.2800 - Val loss: 0.3344 - Test loss: 0.3605 - Train acc:= 90.13% - Val acc:= 87.64% - Test acc:= 87.18%\n",
      "Number of Simulation = 1 - Number of Epoch = 90\n",
      "Train loss:= 0.2703 - Val loss: 0.3294 - Test loss: 0.3559 - Train acc:= 90.41% - Val acc:= 87.87% - Test acc:= 87.29%\n",
      "Number of Simulation = 1 - Number of Epoch = 100\n",
      "Train loss:= 0.2615 - Val loss: 0.3254 - Test loss: 0.3520 - Train acc:= 90.76% - Val acc:= 88.02% - Test acc:= 87.45%\n",
      "\n",
      "Total time taken (in seconds): 1193.75\n",
      "Number of Simulation = 2 - Number of Epoch = 10\n",
      "Train loss:= 0.4312 - Val loss: 0.4486 - Test loss: 0.4618 - Train acc:= 85.15% - Val acc:= 84.08% - Test acc:= 83.56%\n",
      "Number of Simulation = 2 - Number of Epoch = 20\n",
      "Train loss:= 0.3779 - Val loss: 0.4025 - Test loss: 0.4182 - Train acc:= 86.86% - Val acc:= 85.59% - Test acc:= 84.75%\n",
      "Number of Simulation = 2 - Number of Epoch = 30\n",
      "Train loss:= 0.3482 - Val loss: 0.3794 - Test loss: 0.3964 - Train acc:= 87.77% - Val acc:= 86.41% - Test acc:= 85.66%\n",
      "Number of Simulation = 2 - Number of Epoch = 40\n",
      "Train loss:= 0.3268 - Val loss: 0.3646 - Test loss: 0.3819 - Train acc:= 88.41% - Val acc:= 87.04% - Test acc:= 86.44%\n",
      "Number of Simulation = 2 - Number of Epoch = 50\n",
      "Train loss:= 0.3101 - Val loss: 0.3538 - Test loss: 0.3716 - Train acc:= 89.04% - Val acc:= 87.35% - Test acc:= 86.92%\n",
      "Number of Simulation = 2 - Number of Epoch = 60\n",
      "Train loss:= 0.2961 - Val loss: 0.3457 - Test loss: 0.3638 - Train acc:= 89.50% - Val acc:= 87.60% - Test acc:= 87.19%\n",
      "Number of Simulation = 2 - Number of Epoch = 70\n",
      "Train loss:= 0.2840 - Val loss: 0.3394 - Test loss: 0.3574 - Train acc:= 89.93% - Val acc:= 87.76% - Test acc:= 87.46%\n",
      "Number of Simulation = 2 - Number of Epoch = 80\n",
      "Train loss:= 0.2733 - Val loss: 0.3342 - Test loss: 0.3520 - Train acc:= 90.27% - Val acc:= 87.98% - Test acc:= 87.54%\n",
      "Number of Simulation = 2 - Number of Epoch = 90\n",
      "Train loss:= 0.2634 - Val loss: 0.3299 - Test loss: 0.3475 - Train acc:= 90.62% - Val acc:= 88.08% - Test acc:= 87.68%\n",
      "Number of Simulation = 2 - Number of Epoch = 100\n",
      "Train loss:= 0.2545 - Val loss: 0.3263 - Test loss: 0.3439 - Train acc:= 91.00% - Val acc:= 88.19% - Test acc:= 87.82%\n",
      "\n",
      "Total time taken (in seconds): 1036.93\n",
      "Number of Simulation = 3 - Number of Epoch = 10\n",
      "Train loss:= 0.4360 - Val loss: 0.4492 - Test loss: 0.4697 - Train acc:= 84.84% - Val acc:= 83.85% - Test acc:= 83.27%\n",
      "Number of Simulation = 3 - Number of Epoch = 20\n",
      "Train loss:= 0.3839 - Val loss: 0.4039 - Test loss: 0.4277 - Train acc:= 86.53% - Val acc:= 85.59% - Test acc:= 84.62%\n",
      "Number of Simulation = 3 - Number of Epoch = 30\n",
      "Train loss:= 0.3542 - Val loss: 0.3803 - Test loss: 0.4051 - Train acc:= 87.50% - Val acc:= 86.42% - Test acc:= 85.58%\n",
      "Number of Simulation = 3 - Number of Epoch = 40\n",
      "Train loss:= 0.3330 - Val loss: 0.3651 - Test loss: 0.3901 - Train acc:= 88.22% - Val acc:= 87.03% - Test acc:= 86.00%\n",
      "Number of Simulation = 3 - Number of Epoch = 50\n",
      "Train loss:= 0.3164 - Val loss: 0.3542 - Test loss: 0.3791 - Train acc:= 88.75% - Val acc:= 87.42% - Test acc:= 86.45%\n",
      "Number of Simulation = 3 - Number of Epoch = 60\n",
      "Train loss:= 0.3023 - Val loss: 0.3457 - Test loss: 0.3705 - Train acc:= 89.19% - Val acc:= 87.81% - Test acc:= 86.82%\n",
      "Number of Simulation = 3 - Number of Epoch = 70\n",
      "Train loss:= 0.2901 - Val loss: 0.3388 - Test loss: 0.3634 - Train acc:= 89.62% - Val acc:= 87.95% - Test acc:= 87.05%\n",
      "Number of Simulation = 3 - Number of Epoch = 80\n",
      "Train loss:= 0.2793 - Val loss: 0.3330 - Test loss: 0.3575 - Train acc:= 89.98% - Val acc:= 88.07% - Test acc:= 87.26%\n",
      "Number of Simulation = 3 - Number of Epoch = 90\n",
      "Train loss:= 0.2695 - Val loss: 0.3281 - Test loss: 0.3524 - Train acc:= 90.41% - Val acc:= 88.30% - Test acc:= 87.44%\n",
      "Number of Simulation = 3 - Number of Epoch = 100\n",
      "Train loss:= 0.2605 - Val loss: 0.3240 - Test loss: 0.3481 - Train acc:= 90.75% - Val acc:= 88.35% - Test acc:= 87.56%\n",
      "\n",
      "Total time taken (in seconds): 970.46\n",
      "Number of Simulation = 4 - Number of Epoch = 10\n",
      "Train loss:= 0.4259 - Val loss: 0.4398 - Test loss: 0.4596 - Train acc:= 85.13% - Val acc:= 84.77% - Test acc:= 83.61%\n",
      "Number of Simulation = 4 - Number of Epoch = 20\n",
      "Train loss:= 0.3763 - Val loss: 0.3982 - Test loss: 0.4207 - Train acc:= 86.90% - Val acc:= 86.05% - Test acc:= 84.90%\n",
      "Number of Simulation = 4 - Number of Epoch = 30\n",
      "Train loss:= 0.3472 - Val loss: 0.3758 - Test loss: 0.4001 - Train acc:= 87.79% - Val acc:= 86.81% - Test acc:= 85.66%\n",
      "Number of Simulation = 4 - Number of Epoch = 40\n",
      "Train loss:= 0.3263 - Val loss: 0.3614 - Test loss: 0.3866 - Train acc:= 88.44% - Val acc:= 87.29% - Test acc:= 86.37%\n",
      "Number of Simulation = 4 - Number of Epoch = 50\n",
      "Train loss:= 0.3098 - Val loss: 0.3511 - Test loss: 0.3763 - Train acc:= 89.00% - Val acc:= 87.57% - Test acc:= 86.69%\n",
      "Number of Simulation = 4 - Number of Epoch = 60\n",
      "Train loss:= 0.2960 - Val loss: 0.3436 - Test loss: 0.3680 - Train acc:= 89.48% - Val acc:= 87.83% - Test acc:= 86.84%\n",
      "Number of Simulation = 4 - Number of Epoch = 70\n",
      "Train loss:= 0.2840 - Val loss: 0.3373 - Test loss: 0.3613 - Train acc:= 89.92% - Val acc:= 87.99% - Test acc:= 87.11%\n",
      "Number of Simulation = 4 - Number of Epoch = 80\n",
      "Train loss:= 0.2732 - Val loss: 0.3320 - Test loss: 0.3557 - Train acc:= 90.30% - Val acc:= 88.11% - Test acc:= 87.22%\n",
      "Number of Simulation = 4 - Number of Epoch = 90\n",
      "Train loss:= 0.2634 - Val loss: 0.3278 - Test loss: 0.3510 - Train acc:= 90.71% - Val acc:= 88.24% - Test acc:= 87.34%\n",
      "Number of Simulation = 4 - Number of Epoch = 100\n",
      "Train loss:= 0.2545 - Val loss: 0.3244 - Test loss: 0.3471 - Train acc:= 91.02% - Val acc:= 88.26% - Test acc:= 87.41%\n",
      "\n",
      "Total time taken (in seconds): 974.03\n",
      "Number of Simulation = 5 - Number of Epoch = 10\n",
      "Train loss:= 0.4332 - Val loss: 0.4485 - Test loss: 0.4674 - Train acc:= 84.97% - Val acc:= 84.25% - Test acc:= 83.82%\n",
      "Number of Simulation = 5 - Number of Epoch = 20\n",
      "Train loss:= 0.3830 - Val loss: 0.4042 - Test loss: 0.4249 - Train acc:= 86.68% - Val acc:= 86.11% - Test acc:= 85.26%\n",
      "Number of Simulation = 5 - Number of Epoch = 30\n",
      "Train loss:= 0.3538 - Val loss: 0.3803 - Test loss: 0.4022 - Train acc:= 87.64% - Val acc:= 86.80% - Test acc:= 86.01%\n",
      "Number of Simulation = 5 - Number of Epoch = 40\n",
      "Train loss:= 0.3328 - Val loss: 0.3647 - Test loss: 0.3875 - Train acc:= 88.36% - Val acc:= 87.36% - Test acc:= 86.41%\n",
      "Number of Simulation = 5 - Number of Epoch = 50\n",
      "Train loss:= 0.3163 - Val loss: 0.3533 - Test loss: 0.3768 - Train acc:= 88.84% - Val acc:= 87.58% - Test acc:= 86.74%\n",
      "Number of Simulation = 5 - Number of Epoch = 60\n",
      "Train loss:= 0.3025 - Val loss: 0.3448 - Test loss: 0.3687 - Train acc:= 89.34% - Val acc:= 87.83% - Test acc:= 86.93%\n",
      "Number of Simulation = 5 - Number of Epoch = 70\n",
      "Train loss:= 0.2904 - Val loss: 0.3381 - Test loss: 0.3621 - Train acc:= 89.74% - Val acc:= 87.99% - Test acc:= 87.13%\n",
      "Number of Simulation = 5 - Number of Epoch = 80\n",
      "Train loss:= 0.2797 - Val loss: 0.3326 - Test loss: 0.3568 - Train acc:= 90.11% - Val acc:= 88.05% - Test acc:= 87.39%\n",
      "Number of Simulation = 5 - Number of Epoch = 90\n",
      "Train loss:= 0.2698 - Val loss: 0.3279 - Test loss: 0.3524 - Train acc:= 90.47% - Val acc:= 88.15% - Test acc:= 87.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 5 - Number of Epoch = 100\n",
      "Train loss:= 0.2610 - Val loss: 0.3242 - Test loss: 0.3490 - Train acc:= 90.81% - Val acc:= 88.22% - Test acc:= 87.57%\n",
      "\n",
      "Total time taken (in seconds): 981.37\n"
     ]
    }
   ],
   "source": [
    "# Set number of simulations and epochs\n",
    "result_ten = []\n",
    "NUM_SIM = 10\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "for num_sim in range(NUM_SIM):\n",
    "    np.random.seed(num_sim)\n",
    "    tf.random.set_seed(num_sim)\n",
    "    '''\n",
    "    Initialize model using GPU or load an exitsing MLP\n",
    "    '''\n",
    "    mlp_custom = MLP(size_input, size_hidden1, size_hidden2, size_output, device='GPU',\\\n",
    "                 regularizer='l2', R_lambda = 1e-4, drop_prob=0.)\n",
    "\n",
    "    time_start = time.time()\n",
    "    hyperparams = {'t':1, 'lr':2e-5}\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*num_sim).batch(128)\n",
    "\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = mlp_custom.forward(inputs)\n",
    "\n",
    "            #use custom optimizer to train the model\n",
    "            mlp_custom.backward(inputs, outputs, hyperparams,'custom')\n",
    "            hyperparams['t'] += 1\n",
    "        \n",
    "        if (epoch + 1)%10 == 0:\n",
    "            #compute the result for the current epoch\n",
    "            logits = mlp_custom.forward(X_train)\n",
    "            train_loss = np.sum(mlp_custom.loss(logits, y_train))/len(y_train)\n",
    "            train_acc = mlp_custom.accuracy(logits,y_train)/len(y_train)\n",
    "\n",
    "            logits = mlp_custom.forward(X_val)\n",
    "            val_loss = np.sum(mlp_custom.loss(logits, y_val))/len(y_val)\n",
    "            val_acc = mlp_custom.accuracy(logits,y_val)/len(y_val)\n",
    "            \n",
    "            logits = mlp_custom.forward(X_test)\n",
    "            test_loss = np.sum(mlp_custom.loss(logits, y_test))/len(y_test)\n",
    "            test_acc = mlp_custom.accuracy(logits,y_test)/len(y_test)\n",
    "            \n",
    "            print('Number of Simulation = {} - Number of Epoch = {}'.format(num_sim+1, epoch + 1))\n",
    "            print('Train loss:= {:.4f} - Val loss: {:.4f} - Test loss: {:.4f} - Train acc:= {:.2%} - Val acc:= {:.2%} - Test acc:= {:.2%}'\\\n",
    "                  .format(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc))\n",
    "            \n",
    "    time_taken = time.time() - time_start\n",
    "    result_ten.append([train_acc,val_acc,test_acc,time_taken])\n",
    "    print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb3a31b",
   "metadata": {},
   "source": [
    "Train Model with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba2c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of simulations and epochs\n",
    "NUM_SIM = 10\n",
    "NUM_EPOCHS = 200\n",
    "result_ten_SGD = []\n",
    "\n",
    "for num_sim in range(NUM_SIM):\n",
    "    np.random.seed(num_sim)\n",
    "    tf.random.set_seed(num_sim)\n",
    "    '''\n",
    "    Initialize model using GPU or load an exitsing MLP\n",
    "    '''\n",
    "    mlp_SGD = MLP(size_input, size_hidden1, size_hidden2, size_output, device='GPU',\\\n",
    "                 regularizer=None, R_lambda = 1e-4, drop_prob=0.)\n",
    "\n",
    "    time_start = time.time()\n",
    "    hyperparams = {'t':1, 'lr':2e-5}\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*num_sim).batch(128)\n",
    "\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = mlp_SGD.forward(inputs)\n",
    "\n",
    "            #use custom optimizer to train the model\n",
    "            mlp_SGD.backward(inputs, outputs, hyperparams,'sgd')\n",
    "            hyperparams['t'] += 1\n",
    "        \n",
    "        if (epoch + 1)%10 == 0:\n",
    "            #compute the result for the current epoch\n",
    "            logits = mlp_SGD.forward(X_train)\n",
    "            train_loss = np.sum(mlp_SGD.loss(logits, y_train))/len(y_train)\n",
    "            train_acc = mlp_SGD.accuracy(logits,y_train)/len(y_train)\n",
    "\n",
    "            logits = mlp_SGD.forward(X_val)\n",
    "            val_loss = np.sum(mlp_SGD.loss(logits, y_val))/len(y_val)\n",
    "            val_acc = mlp_SGD.accuracy(logits,y_val)/len(y_val)\n",
    "            \n",
    "            logits = mlp_SGD.forward(X_test)\n",
    "            test_loss = np.sum(mlp_SGD.loss(logits, y_test))/len(y_test)\n",
    "            test_acc = mlp_SGD.accuracy(logits,y_test)/len(y_test)\n",
    "            \n",
    "            print('Number of Simulation = {} - Number of Epoch = {}'.format(num_sim+1, epoch + 1))\n",
    "            print('Train loss:= {:.4f} - Val loss: {:.4f} - Test loss: {:.4f} - Train acc:= {:.2%} - Val acc:= {:.2%} - Test acc:= {:.2%}'\\\n",
    "                  .format(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc))\n",
    "            \n",
    "    time_taken = time.time() - time_start\n",
    "    result_ten_SGD.append([train_acc,val_acc,test_acc,time_taken])\n",
    "    print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825699ab",
   "metadata": {},
   "source": [
    "Train Model with RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "431f3cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 1 - Number of Epoch = 10\n",
      "Train loss:= 0.3873 - Val loss: 0.4128 - Test loss: 0.4352 - Train acc:= 86.35% - Val acc:= 85.59% - Test acc:= 84.62%\n",
      "Number of Simulation = 1 - Number of Epoch = 20\n",
      "Train loss:= 0.3400 - Val loss: 0.3772 - Test loss: 0.4017 - Train acc:= 88.03% - Val acc:= 86.75% - Test acc:= 85.84%\n",
      "Number of Simulation = 1 - Number of Epoch = 30\n",
      "Train loss:= 0.3134 - Val loss: 0.3642 - Test loss: 0.3876 - Train acc:= 88.98% - Val acc:= 87.25% - Test acc:= 86.56%\n",
      "Number of Simulation = 1 - Number of Epoch = 40\n",
      "Train loss:= 0.2948 - Val loss: 0.3576 - Test loss: 0.3816 - Train acc:= 89.64% - Val acc:= 87.58% - Test acc:= 86.89%\n",
      "Number of Simulation = 1 - Number of Epoch = 50\n",
      "Train loss:= 0.2804 - Val loss: 0.3546 - Test loss: 0.3803 - Train acc:= 90.10% - Val acc:= 87.71% - Test acc:= 87.02%\n",
      "Number of Simulation = 1 - Number of Epoch = 60\n",
      "Train loss:= 0.2680 - Val loss: 0.3534 - Test loss: 0.3813 - Train acc:= 90.56% - Val acc:= 87.80% - Test acc:= 87.25%\n",
      "Number of Simulation = 1 - Number of Epoch = 70\n",
      "Train loss:= 0.2591 - Val loss: 0.3564 - Test loss: 0.3851 - Train acc:= 90.96% - Val acc:= 87.91% - Test acc:= 87.30%\n",
      "Number of Simulation = 1 - Number of Epoch = 80\n",
      "Train loss:= 0.2519 - Val loss: 0.3598 - Test loss: 0.3887 - Train acc:= 91.16% - Val acc:= 88.14% - Test acc:= 87.35%\n",
      "Number of Simulation = 1 - Number of Epoch = 90\n",
      "Train loss:= 0.2467 - Val loss: 0.3666 - Test loss: 0.3968 - Train acc:= 91.44% - Val acc:= 88.02% - Test acc:= 87.45%\n",
      "Number of Simulation = 1 - Number of Epoch = 100\n",
      "Train loss:= 0.2437 - Val loss: 0.3749 - Test loss: 0.4072 - Train acc:= 91.64% - Val acc:= 87.87% - Test acc:= 87.21%\n",
      "Number of Simulation = 1 - Number of Epoch = 110\n",
      "Train loss:= 0.2391 - Val loss: 0.3852 - Test loss: 0.4192 - Train acc:= 91.92% - Val acc:= 87.90% - Test acc:= 87.21%\n",
      "Number of Simulation = 1 - Number of Epoch = 120\n",
      "Train loss:= 0.2351 - Val loss: 0.3908 - Test loss: 0.4278 - Train acc:= 92.15% - Val acc:= 87.87% - Test acc:= 87.29%\n",
      "Number of Simulation = 1 - Number of Epoch = 130\n",
      "Train loss:= 0.2323 - Val loss: 0.3980 - Test loss: 0.4360 - Train acc:= 92.27% - Val acc:= 87.68% - Test acc:= 87.23%\n",
      "Number of Simulation = 1 - Number of Epoch = 140\n",
      "Train loss:= 0.2299 - Val loss: 0.4039 - Test loss: 0.4437 - Train acc:= 92.33% - Val acc:= 87.72% - Test acc:= 87.11%\n",
      "Number of Simulation = 1 - Number of Epoch = 150\n",
      "Train loss:= 0.2302 - Val loss: 0.4160 - Test loss: 0.4532 - Train acc:= 92.39% - Val acc:= 87.41% - Test acc:= 87.02%\n",
      "Number of Simulation = 1 - Number of Epoch = 160\n",
      "Train loss:= 0.2290 - Val loss: 0.4230 - Test loss: 0.4622 - Train acc:= 92.46% - Val acc:= 87.42% - Test acc:= 86.86%\n",
      "Number of Simulation = 1 - Number of Epoch = 170\n",
      "Train loss:= 0.2308 - Val loss: 0.4340 - Test loss: 0.4741 - Train acc:= 92.48% - Val acc:= 87.73% - Test acc:= 86.85%\n",
      "Number of Simulation = 1 - Number of Epoch = 180\n",
      "Train loss:= 0.2317 - Val loss: 0.4497 - Test loss: 0.4862 - Train acc:= 92.54% - Val acc:= 87.55% - Test acc:= 86.73%\n",
      "Number of Simulation = 1 - Number of Epoch = 190\n",
      "Train loss:= 0.2330 - Val loss: 0.4602 - Test loss: 0.5052 - Train acc:= 92.57% - Val acc:= 87.75% - Test acc:= 86.67%\n",
      "Number of Simulation = 1 - Number of Epoch = 200\n",
      "Train loss:= 0.2285 - Val loss: 0.4626 - Test loss: 0.5090 - Train acc:= 92.66% - Val acc:= 87.76% - Test acc:= 86.56%\n",
      "\n",
      "Total time taken (in seconds): 1381.61\n"
     ]
    }
   ],
   "source": [
    "# Set number of simulations and epochs\n",
    "NUM_SIM = 10\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "for num_sim in range(NUM_SIM):\n",
    "    np.random.seed(num_sim)\n",
    "    tf.random.set_seed(num_sim)\n",
    "    '''\n",
    "    Initialize model using GPU or load an exitsing MLP\n",
    "    '''\n",
    "    mlp_rms = MLP(size_input, size_hidden1, size_hidden2, size_output, device='GPU',\\\n",
    "                 regularizer='l2', R_lambda = 1e-4, drop_prob=0.)\n",
    "\n",
    "    time_start = time.time()\n",
    "    hyperparams = {'t':1, 'lr':2e-5}\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*num_sim).batch(128)\n",
    "\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = mlp_rms.forward(inputs)\n",
    "\n",
    "            #use custom optimizer to train the model\n",
    "            mlp_rms.backward(inputs, outputs, hyperparams,'RMSprop')\n",
    "            hyperparams['t'] += 1\n",
    "        \n",
    "        if (epoch + 1)%10 == 0:\n",
    "            #compute the result for the current epoch\n",
    "            logits = mlp_rms.forward(X_train)\n",
    "            train_loss = np.sum(mlp_rms.loss(logits, y_train))/len(y_train)\n",
    "            train_acc = mlp_rms.accuracy(logits,y_train)/len(y_train)\n",
    "\n",
    "\n",
    "            logits = mlp_rms.forward(X_val)\n",
    "            val_loss = np.sum(mlp_rms.loss(logits, y_val))/len(y_val)\n",
    "            val_acc = mlp_rms.accuracy(logits,y_val)/len(y_val)\n",
    "\n",
    "            \n",
    "            logits = mlp_rms.forward(X_test)\n",
    "            test_loss = np.sum(mlp_rms.loss(logits, y_test))/len(y_test)\n",
    "            test_acc = mlp_rms.accuracy(logits,y_test)/len(y_test)\n",
    "\n",
    "            \n",
    "            print('Number of Simulation = {} - Number of Epoch = {}'.format(num_sim+1, epoch + 1))\n",
    "            print('Train loss:= {:.4f} - Val loss: {:.4f} - Test loss: {:.4f} - Train acc:= {:.2%} - Val acc:= {:.2%} - Test acc:= {:.2%}'\\\n",
    "                  .format(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc))\n",
    "            \n",
    "    time_taken = time.time() - time_start \n",
    "    print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aab997",
   "metadata": {},
   "source": [
    "Train Model with ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0c62f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Simulation = 1 - Number of Epoch = 10\n",
      "Train loss:= 0.4798 - Val loss: 0.4926 - Test loss: 0.5140 - Train acc:= 83.43% - Val acc:= 82.87% - Test acc:= 81.99%\n",
      "Number of Simulation = 1 - Number of Epoch = 20\n",
      "Train loss:= 0.4171 - Val loss: 0.4377 - Test loss: 0.4606 - Train acc:= 85.50% - Val acc:= 84.55% - Test acc:= 83.80%\n",
      "Number of Simulation = 1 - Number of Epoch = 30\n",
      "Train loss:= 0.3866 - Val loss: 0.4114 - Test loss: 0.4347 - Train acc:= 86.51% - Val acc:= 85.72% - Test acc:= 84.46%\n",
      "Number of Simulation = 1 - Number of Epoch = 40\n",
      "Train loss:= 0.3666 - Val loss: 0.3950 - Test loss: 0.4190 - Train acc:= 87.21% - Val acc:= 86.26% - Test acc:= 85.11%\n",
      "Number of Simulation = 1 - Number of Epoch = 50\n",
      "Train loss:= 0.3516 - Val loss: 0.3834 - Test loss: 0.4081 - Train acc:= 87.73% - Val acc:= 86.51% - Test acc:= 85.57%\n",
      "Number of Simulation = 1 - Number of Epoch = 60\n",
      "Train loss:= 0.3394 - Val loss: 0.3749 - Test loss: 0.4006 - Train acc:= 88.08% - Val acc:= 86.73% - Test acc:= 85.98%\n",
      "Number of Simulation = 1 - Number of Epoch = 70\n",
      "Train loss:= 0.3291 - Val loss: 0.3686 - Test loss: 0.3947 - Train acc:= 88.46% - Val acc:= 86.85% - Test acc:= 86.30%\n",
      "Number of Simulation = 1 - Number of Epoch = 80\n",
      "Train loss:= 0.3203 - Val loss: 0.3634 - Test loss: 0.3902 - Train acc:= 88.73% - Val acc:= 87.09% - Test acc:= 86.45%\n",
      "Number of Simulation = 1 - Number of Epoch = 90\n",
      "Train loss:= 0.3126 - Val loss: 0.3594 - Test loss: 0.3870 - Train acc:= 88.99% - Val acc:= 87.24% - Test acc:= 86.60%\n",
      "Number of Simulation = 1 - Number of Epoch = 100\n",
      "Train loss:= 0.3057 - Val loss: 0.3562 - Test loss: 0.3842 - Train acc:= 89.21% - Val acc:= 87.38% - Test acc:= 86.72%\n",
      "\n",
      "Total time taken (in seconds): 590.07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set number of simulations and epochs\n",
    "NUM_SIM = 10\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "for num_sim in range(NUM_SIM):\n",
    "    np.random.seed(num_sim)\n",
    "    tf.random.set_seed(num_sim)\n",
    "    '''\n",
    "    Initialize model using GPU or load an exitsing MLP\n",
    "    '''\n",
    "    mlp_adam = MLP(size_input, size_hidden1, size_hidden2, size_output, device='GPU',\\\n",
    "                 regularizer='l2', R_lambda = 1e-4, drop_prob=0.)\n",
    "\n",
    "    time_start = time.time()\n",
    "    hyperparams = {'t':1, 'lr':2e-5}\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*num_sim).batch(128)\n",
    "\n",
    "        for inputs, outputs in train_ds:\n",
    "            preds = mlp_adam.forward(inputs)\n",
    "\n",
    "            #use custom optimizer to train the model\n",
    "            mlp_adam.backward(inputs, outputs, hyperparams,'adam')\n",
    "            hyperparams['t'] += 1\n",
    "        \n",
    "        if (epoch + 1)%10 == 0:\n",
    "            #compute the result for the current epoch\n",
    "            logits = mlp_adam.forward(X_train)\n",
    "            train_loss = np.sum(mlp_adam.loss(logits, y_train))/len(y_train)\n",
    "            train_acc = mlp_adam.accuracy(logits,y_train)/len(y_train)\n",
    "\n",
    "            logits = mlp_adam.forward(X_val)\n",
    "            val_loss = np.sum(mlp_adam.loss(logits, y_val))/len(y_val)\n",
    "            val_acc = mlp_adam.accuracy(logits,y_val)/len(y_val)\n",
    "            \n",
    "            logits = mlp_adam.forward(X_test)\n",
    "            test_loss = np.sum(mlp_adam.loss(logits, y_test))/len(y_test)\n",
    "            test_acc = mlp_adam.accuracy(logits,y_test)/len(y_test)\n",
    "            \n",
    "            print('Number of Simulation = {} - Number of Epoch = {}'.format(num_sim+1, epoch + 1))\n",
    "            print('Train loss:= {:.4f} - Val loss: {:.4f} - Test loss: {:.4f} - Train acc:= {:.2%} - Val acc:= {:.2%} - Test acc:= {:.2%}'\\\n",
    "                  .format(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc))\n",
    "            \n",
    "    time_taken = time.time() - time_start \n",
    "    print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "#save_object(mlp_DIY,'mlp_DIY_dropout.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
